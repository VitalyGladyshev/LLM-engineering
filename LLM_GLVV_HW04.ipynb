{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef538bfe-80e6-4d0a-84eb-103cabc635a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "import torch, gc, os, math, random\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd6b96c5-c6b3-49ca-b6e2-61ae101a43d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "nvmlInit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe370538-ebcc-4bd7-9b42-04faafa20342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "904d2417-aae3-45a4-b185-b671e36d547e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-09 17:10:30 [__init__.py:241] Automatically detected platform cuda.\n",
      "WARNING 09-09 17:10:30 [cuda.py:605] Detected different devices in the system: NVIDIA GeForce RTX 3090 Ti, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c940c92a-e77a-41bb-8da7-0d3705922198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "235d7e66-55cc-498f-a91d-fd84161c54b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aae08193-58ab-4ebb-878f-d13047002e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def gpu_mem(note=\"\"):\n",
    "    if not torch.cuda.is_available():\n",
    "        print(f\"[{note}] No CUDA available.\")\n",
    "        return\n",
    "    torch.cuda.synchronize()\n",
    "    alloc = torch.cuda.memory_allocated() / (1024**3)\n",
    "    resrv = torch.cuda.memory_reserved() / (1024**3)\n",
    "    peak = torch.cuda.max_memory_allocated() / (1024**3)\n",
    "    print(f\"[{note}] allocated={alloc:.2f}GB, reserved={resrv:.2f}GB, peak={peak:.2f}GB\")\n",
    "\n",
    "def nvidia_mem():\n",
    "    if not torch.cuda.is_available():\n",
    "        return\n",
    "\n",
    "    nvmlInit()\n",
    "    h = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(h)\n",
    "    print(f\"NVML used={info.used/(1024**3):.2f}GB / total={info.total/(1024**3):.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53310e3b-f80d-4703-8327-67cc9fea00dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fresh] allocated=0.00GB, reserved=0.00GB, peak=0.00GB\n",
      "NVML used=1.28GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "flush()\n",
    "gpu_mem(\"fresh\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f4099-b723-43af-8b0c-ad13de19e27c",
   "metadata": {},
   "source": [
    "## Модель Meta-Llama-3.1-8B-Instruct-bnb-4bit от Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67ed1d4a-0af9-4216-8d6c-33ee8a9c55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "063a7abe-3de6-423c-89b9-524263697962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клонирование в «Llama-3.1-8B-Instruct»...\n",
      "remote: Enumerating objects: 109, done.\u001b[K\n",
      "remote: Counting objects: 100% (106/106), done.\u001b[K\n",
      "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
      "remote: Total 109 (delta 53), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Получение объектов: 100% (109/109), 2.28 МиБ | 4.57 МиБ/с, готово.\n",
      "Определение изменений: 100% (53/53), готово.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://viv232:hf_xxxxx@huggingface.co/meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b99b98b3-0693-4629-8b1b-b335b334031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"meta-llama/Llama-3.1-8B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fef332-d529-4248-b804-52f43ef279e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Клонирование в «Meta-Llama-3.1-8B-Instruct-bnb-4bit»...\n",
      "remote: Enumerating objects: 131, done.\u001b[K\n",
      "remote: Counting objects: 100% (128/128), done.\u001b[K\n",
      "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
      "remote: Total 131 (delta 44), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
      "Получение объектов: 100% (131/131), 2.30 МиБ | 3.76 МиБ/с, готово.\n",
      "Определение изменений: 100% (44/44), готово.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a947cca9-efef-4fff-bd48-df5455fe38a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b77bc1d-cb60-4a6f-8796-cc11c0f6dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[before load QLoRA] allocated=0.00GB, reserved=0.00GB, peak=0.00GB\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 1024\n",
    "\n",
    "flush()\n",
    "gpu_mem(\"before load QLoRA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241ce030-458e-489f-9251-8738645afb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 2. Max memory: 23.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,    # QLoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d92c9bb0-b82d-44ae-9efb-2f1446b37e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c74494c0-c740-47c4-bda8-6ba576e544c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[after load QLoRA] allocated=5.50GB, reserved=5.52GB, peak=7.02GB\n",
      "NVML used=6.64GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"after load QLoRA\")\n",
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cffbeac3-1aa3-4334-affb-3f56a43b3519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|eot_id|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78cd47d9-c4ef-4190-825e-6fe55aeb749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Чат-шаблон: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Чат-шаблон: {tokenizer.chat_template}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a7642-4e04-4f9d-b021-850f17422119",
   "metadata": {},
   "source": [
    "### Оценка до LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17984ca1-d489-4052-975a-2204a7946368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1193e7b9-9a9d-4049-9b1d-d5f156c506f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_test = [\n",
    "    'Как вкусно приготовить индейку на гриле?',\n",
    "    'Как распознать приближающийся инсульт?',\n",
    "    'Сформулируй основные каноны архитектуры древних цивилизаций',\n",
    "    'Облагать ли страховыми взносами суммы прощенного долга по займу от организации где работает застрахованный?',\n",
    "    'Расскажи мне про Курчатова'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a92eadf6-5744-4e0c-8fa8-f455409b518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "    dialog = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], \n",
    "                                           tokenize=False, \n",
    "                                           add_generation_prompt=True)\n",
    "    inputs = tokenizer(dialog, return_tensors = \"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=300, use_cache=True)\n",
    "    return tokenizer.batch_decode(outputs)[0].split(\"assistant\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84e5b651-9400-4ff1-9ee8-7bf3d6b03690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "\n",
      "Чтобы приготовить вкусную индейку на гриле, следуйте этим рекомендациям:\n",
      "\n",
      "**Приготовление индейки на гриле:**\n",
      "\n",
      "Ингредиенты:\n",
      "\n",
      "*   1 индейка (весом 1,5-2 кг)\n",
      "*   2 столовые ложки оливкового масла\n",
      "*   1 чайная ложка соли\n",
      "*   1 чайная ложка черного перца\n",
      "*   1 чайная ложка паприки\n",
      "*   1 чайная ложка чеснока, измельченного\n",
      "*   1 луковица, измельченная\n",
      "*   2 веточки розмарина (по желанию)\n",
      "*   1 лимон, нарезанный (по желанию)\n",
      "\n",
      "**Подготовка индейки:**\n",
      "\n",
      "1.  Налейте индейку в форму для гриля или на противень.\n",
      "2.  В миске смешайте оливковое масло, соль, черный перец, паприку, чеснок и лук.\n",
      "3.  Нанесите смесь на индейку, равномерно распределив ее по всей поверхности.\n",
      "4.  Добавьте розмарин и лимон по желанию.\n",
      "\n",
      "**Гриляние индейки:**\n",
      "\n",
      "1.  Разогрейте гриль до средней температуры.\n",
      "2.  П\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Приближающийся инсульт может быть сложно распознать, особенно в его ранних стадиях. Однако существуют некоторые признаки и симптомы, которые могут указывать на потенциальный инсульт. Вот некоторые из них:\n",
      "\n",
      "1.  **Нарушение речи**: проблемы с речью, такие как затруднение произношения слов, трудности с пониманием или непонимание речи других людей.\n",
      "2.  **Нарушение зрения**: потеря зрения в одном или обоих глазах, слепота или двоение в глазах.\n",
      "3.  **Нарушение движений**: слабость или паралич в одной или обеих руках или ногах, трудности с ходьбой или потеря равновесия.\n",
      "4.  **Нарушение чувствительности**: потеря чувствительности в одной или обеих руках или ногах.\n",
      "5.  **Головная боль**: сильная головная боль, особенно в одной стороне головы.\n",
      "6.  **Синяя кожа**: синяя кожа на лице или конечностях.\n",
      "7.  **Нарушение памяти**: проблемы с памятью или внимание.\n",
      "8.  **Нарушение настроения**: перепады настроения или агрессивное поведение.\n",
      "9.  **Нарушение мыш\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Архитектура древних цивилизаций включала в себя различные стили и принципы, но существуют некоторые общие каноны, которые были распространены среди различных культур. Вот некоторые основные каноны:\n",
      "\n",
      "1. **Симметрия и баланс**: Архитектура древних цивилизаций часто включала симметричные композиции, которые создавали чувство баланса и гармонии. Это могло выражаться в симметричной планировке зданий, фасадов и интерьеров.\n",
      "2. **Использование природных материалов**: Древние цивилизации часто использовали местные материалы, такие как камень, дерево, глина и кирпичи, для постройки зданий и сооружений.\n",
      "3. **Использование геометрических форм**: Архитектура древних цивилизаций часто включала использование геометрических форм, таких как круги, квадраты, треугольники и ромбы, для создания композиций и орнаментов.\n",
      "4. **Использование орнаментов**: Древние цивилизации часто использовали орнаменты, такие как узоры, мотивы и символы, для украшения зданий и сооружений.\n",
      "5. **Связь с природой**: Архитектура\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Вопрос о взимании страховых взносов на суммы прощенного долга по займу от организации, где работает застрахованный, является сложным и зависит от ряда факторов. В России действуют следующие правила:\n",
      "\n",
      "1. **Законодательство**: Согласно статье 16.1 Налогового кодекса РФ, страховые взносы взимаются на доходы, полученные от предпринимательской деятельности, а также на суммы, полученные от реализации имущества, если оно не является основным имуществом организации. Однако, если организация предоставляет займы своим сотрудникам, то эти займы не должны учитываться в качестве доходов, если они не были возвращены в течение определенного срока.\n",
      "\n",
      "2. **Правило \"возвратности\"**: Согласно п. 5.15 Положения о страховых взносах, если организация предоставляет займы своим сотрудникам и не требует возврата, эти суммы не учитываются в качестве доходов и не облагаются страховыми взносами. Однако, если организация требует возврата, то эти суммы учитываются в качестве доходов и облагаются страховыми взносами.\n",
      "\n",
      "3. **Прокредитование сотрудников**: Если организация предоставляет своим сотрудникам кредиты под залог недвижимости или другого имущества, то\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Курчатов - это русский учёный и изобретатель, который внес значительный вклад в развитие радиотехники и радиолокации. Он родился в 1903 году и умер в 1991 году.\n",
      "\n",
      "В годы Великой Отечественной войны Курчатов работал в области радиолокации и радиотехники, разрабатывая системы радиолокации и радиопередачи. После войны он продолжил свою работу в области радиотехники и радиолокации, в частности, в области разработки радиолокационных систем для военных целей.\n",
      "\n",
      "Курчатов также работал в области радиолокации и радиотехники в области гражданских приложений, например, в области телевидения и радиотелефонии. Он был одним из первых, кто использовал радиолокацию для навигации в гражданском воздухоплавании.\n",
      "\n",
      "Курчатов был также известен как организатор научно-технического труда и преподаватель. Он преподавал в высших учебных заведениях и был ректором Московского института инженеров радиотехники, электронной техники и автоматики.\n",
      "\n",
      "В 1951 году Курчатов был удостоен Сталинской премии за разработку радиолокационной системы \"Свема\". В 1960 году\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd757c80-f9bc-40eb-8332-f3ecbe8c9ee0",
   "metadata": {},
   "source": [
    "lm_eval --model hf \\\n",
    "    --model_args pretrained=unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit,dtype=\"float\" \\\n",
    "    --tasks truthfulqa_ru_mc1 \\\n",
    "    --device cuda:0 \\\n",
    "    --batch_size auto:4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d58037e0-e997-4caf-a3f9-3ad54c1743ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "INFO 09-09 14:33:34 [__init__.py:241] Automatically detected platform cuda.\n",
      "WARNING 09-09 14:33:34 [cuda.py:605] Detected different devices in the system: NVIDIA GeForce RTX 3090 Ti, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n",
      "2025-09-09:14:33:36 INFO     [__main__:446] Selected Tasks: ['truthfulqa_ru_mc1']\n",
      "2025-09-09:14:33:36 WARNING  [evaluator:172] pretrained=pretrained=unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit,dtype=float appears to be an instruct or chat variant but chat template is\n",
      "        not applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-09-09:14:33:36 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-09-09:14:33:36 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit', 'dtype': 'float'}\n",
      "2025-09-09:14:33:36 INFO     [models.huggingface:147] Using device 'cuda:0'\n",
      "2025-09-09:14:33:37 INFO     [models.huggingface:414] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "2025-09-09:14:33:42 INFO     [api.task:434] Building contexts for truthfulqa_ru_mc1 on rank 0...\n",
      "100%|█████████████████████████████████████| 788/788 [00:00<00:00, 173706.39it/s]\n",
      "2025-09-09:14:33:42 INFO     [evaluator:574] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                  | 0/3961 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 22\n",
      "Running loglikelihood requests:  24%|█▉      | 948/3961 [01:42<02:50, 17.65it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  24%|█▉      | 969/3961 [01:58<02:49, 17.65it/s]Determined largest batch size: 25\n",
      "Running loglikelihood requests:  49%|███▍   | 1950/3961 [03:09<01:48, 18.51it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  50%|███▍   | 1974/3961 [03:28<01:47, 18.51it/s]Determined largest batch size: 25\n",
      "Running loglikelihood requests:  74%|█████▏ | 2926/3961 [04:32<00:53, 19.41it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  74%|█████▏ | 2950/3961 [04:48<00:52, 19.41it/s]Determined largest batch size: 25\n",
      "Running loglikelihood requests:  99%|██████▉| 3930/3961 [05:54<00:01, 20.33it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests: 100%|██████▉| 3954/3961 [06:08<00:00, 20.33it/s]Determined largest batch size: 32\n",
      "Running loglikelihood requests: 100%|███████| 3961/3961 [06:21<00:00, 10.39it/s]\n",
      "fatal: не найден git репозиторий (или один из родительских каталогов): .git\n",
      "2025-09-09:14:40:06 INFO     [loggers.evaluation_tracker:280] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit,dtype=float), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (22,25,25,25,32)\n",
      "|      Tasks      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|-----------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_ru_mc1|      1|none  |     0|acc   |↑  |0.3185|±  |0.0166|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash ./lm-evaluation-harness/run_lmesh.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0319e902-0ec2-4033-9f46-323e7dd7ed15",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c95b763-6617-4602-be3b-86195ffcceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 151822/151822 [00:00<00:00, 171669.37 examples/s]\n",
      "Generating test split: 100%|██████████| 3291/3291 [00:00<00:00, 157742.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "vikhr_dataset = load_dataset(\"Vikhrmodels/GrandMaster-PRO-MAX\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6783cb38-5c5a-45fa-afcb-64aeccafa5f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'conversation', 'prompt_tokens', 'answer_tokens', 'cluster', 'prompt_lang', 'answer_lang'],\n",
       "    num_rows: 151822\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vikhr_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6c4ae45-abb2-44e9-a5a7-7a9be64a368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_russian(example):\n",
    "    return example['prompt_lang'] == 'ru' and example['answer_lang'] == 'ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "646cb801-adcc-4f5c-8f7f-97af3321defb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 151822/151822 [00:01<00:00, 101434.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "vikhr_dataset = vikhr_dataset.filter(filter_russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8322674c-7d75-4b69-a20c-9cae524c8f2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86295"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vikhr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4033d046-bb07-41d3-bed5-8254711b520b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'слушай, у меня тут возникла задачка по архитектуре компьютера, и я не могу в ней разобраться. мне нужно разработать алгоритм, который оптимизирует доступ к кэш-памяти в многоядерном процессоре для параллельных вычислений. ты не мог бы помочь с этим? вот как я представляю задачу:\\n\\n1. имеется многоядерный процессор с общим кэшем второго уровня.\\n2. необходимо минимизировать промахи кэша при параллельном выполнении нескольких интенсивных задач по обработке данных.\\n3. алгоритм должен распределять данные таким образом, чтобы максимально использовать преимущества локальности данных и минимизировать конфликтные промахи.\\n\\nможет есть идеи, как это можно организовать на уровне алгоритма? буду рад любым предложениям и советам!',\n",
       "  'role': 'user'},\n",
       " {'content': 'Ваша задача действительно интересная и актуальная в контексте современных многоядерных процессоров. Для оптимизации доступа к кэш-памяти важно учитывать такие аспекты, как пространственная и временная локальность данных, а также избегать конфликтов при кэшировании. Вот основные шаги, которые помогут в разработке алгоритма:\\n\\n### Шаг 1: Анализ задач\\nПрежде всего, нужно проанализировать характеристики задач, которые будут выполняться параллельно. Важно понять, какие данные используются и как часто они запрашиваются.\\n\\n### Шаг 2: Разработка стратегии распределения данных\\nДалее, необходимо разработать стратегию, которая позволит распределить данные таким образом, чтобы максимально использовать преимущества пространственной и временной локальности. Это может включать в себя:\\n\\n- **Группировка связанных данных**: чтобы при обращении к одному элементу данных рядом находились и другие часто используемые элементы.\\n- **Разделение данных между ядрами**: чтобы каждое ядро работало с уникальным набором данных и минимизировать конфликтные промахи.\\n\\n### Шаг 3: Разбиение данных на блоки\\nПосле того как стратегия распределения данных разработана, следует разбить данные на блоки оптимального размера для кэширования. В идеале, размер блока должен соответствовать размеру линейки кэша.\\n\\n### Шаг 4: Реализация алгоритма предвыборки (prefetching)\\nРазработайте алгоритм предвыборки данных, который будет загружать данные в кэш заранее, основываясь на предсказании обращений к данным.\\n\\n### Шаг 5: Синхронизация доступа к данным\\nНеобходимо также обеспечить эффективную синхронизацию доступа к данным между ядрами, чтобы избежать ситуаций, когда несколько ядер одновременно обновляют данные в кэше.\\n\\n### Шаг 6: Тестирование и отладка\\nПроведите тестирование алгоритма на различных наборах данных и рабочих нагрузках. Оцените эффективность алгоритма, подсчитывая количество промахов кэша и сравнивая с базовым случаем.\\n\\n### Шаг 7: Итерация и оптимизация\\nНа основе полученных результатов внесите коррективы в алгоритм и повторите тестирование. Ищите возможности для дальнейшей оптимизации.\\n\\nЭти шаги должны помочь вам в разработке эффективного алгоритма оптимизации доступа к кэш-памяти для параллельных вычислений. Успехов в работе!',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vikhr_dataset[1][\"conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cb329e9-73fd-4731-b61c-7d9cc274eb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nслушай, у меня тут возникла задачка по архитектуре компьютера, и я не могу в ней разобраться. мне нужно разработать алгоритм, который оптимизирует доступ к кэш-памяти в многоядерном процессоре для параллельных вычислений. ты не мог бы помочь с этим? вот как я представляю задачу:\\n\\n1. имеется многоядерный процессор с общим кэшем второго уровня.\\n2. необходимо минимизировать промахи кэша при параллельном выполнении нескольких интенсивных задач по обработке данных.\\n3. алгоритм должен распределять данные таким образом, чтобы максимально использовать преимущества локальности данных и минимизировать конфликтные промахи.\\n\\nможет есть идеи, как это можно организовать на уровне алгоритма? буду рад любым предложениям и советам!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nВаша задача действительно интересная и актуальная в контексте современных многоядерных процессоров. Для оптимизации доступа к кэш-памяти важно учитывать такие аспекты, как пространственная и временная локальность данных, а также избегать конфликтов при кэшировании. Вот основные шаги, которые помогут в разработке алгоритма:\\n\\n### Шаг 1: Анализ задач\\nПрежде всего, нужно проанализировать характеристики задач, которые будут выполняться параллельно. Важно понять, какие данные используются и как часто они запрашиваются.\\n\\n### Шаг 2: Разработка стратегии распределения данных\\nДалее, необходимо разработать стратегию, которая позволит распределить данные таким образом, чтобы максимально использовать преимущества пространственной и временной локальности. Это может включать в себя:\\n\\n- **Группировка связанных данных**: чтобы при обращении к одному элементу данных рядом находились и другие часто используемые элементы.\\n- **Разделение данных между ядрами**: чтобы каждое ядро работало с уникальным набором данных и минимизировать конфликтные промахи.\\n\\n### Шаг 3: Разбиение данных на блоки\\nПосле того как стратегия распределения данных разработана, следует разбить данные на блоки оптимального размера для кэширования. В идеале, размер блока должен соответствовать размеру линейки кэша.\\n\\n### Шаг 4: Реализация алгоритма предвыборки (prefetching)\\nРазработайте алгоритм предвыборки данных, который будет загружать данные в кэш заранее, основываясь на предсказании обращений к данным.\\n\\n### Шаг 5: Синхронизация доступа к данным\\nНеобходимо также обеспечить эффективную синхронизацию доступа к данным между ядрами, чтобы избежать ситуаций, когда несколько ядер одновременно обновляют данные в кэше.\\n\\n### Шаг 6: Тестирование и отладка\\nПроведите тестирование алгоритма на различных наборах данных и рабочих нагрузках. Оцените эффективность алгоритма, подсчитывая количество промахов кэша и сравнивая с базовым случаем.\\n\\n### Шаг 7: Итерация и оптимизация\\nНа основе полученных результатов внесите коррективы в алгоритм и повторите тестирование. Ищите возможности для дальнейшей оптимизации.\\n\\nЭти шаги должны помочь вам в разработке эффективного алгоритма оптимизации доступа к кэш-памяти для параллельных вычислений. Успехов в работе!<|eot_id|>'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.apply_chat_template(vikhr_dataset[1][\"conversation\"], tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "44cbfa61-543a-4cf8-8dee-012caede6792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    return {\"text\": tokenizer.apply_chat_template(example[\"conversation\"], tokenize=False)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c682408-5a64-4532-b028-c93af43236b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_prep = vikhr_dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36f54688-5540-48ca-bf28-1237e217f86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(check_data_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5908bdd0-6305-4689-a454-e11d00f8ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 1229.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "check_data = check_data_prep.map(formatting_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9e1ddf2d-2413-4daa-adcc-2b39c8618be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['source', 'conversation', 'prompt_tokens', 'answer_tokens', 'cluster', 'prompt_lang', 'answer_lang', 'text'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb4c4353-dbc3-4472-aee4-d72cf7da5706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'generated/saiga/tagengo/lmsys_pref',\n",
       " 'conversation': [{'content': 'мне очень интересны стратегические игры, и я недавно узнал про игру ним. не мог бы ты объяснить мне стратегию оптимальной игры в ним? и еще, если есть, поделись интересным вариантом игры в крестики-нолики или другие стратегические головоломки, в которые мы могли бы сыграть вместе. как насчет того, чтобы рассмотреть 15 puzzle? мне бы хотелось узнать, есть ли для неё какая-то выигрышная стратегия или подход, который гарантирует победу.',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Расскажу тебе о стратегиях игры в Ним и затрону тему 15 Puzzle.\\n\\n### Стратегия оптимальной игры в Ним\\n\\nИгра Ним — это математическая игра, для которой существует чёткая выигрышная стратегия. Основа стратегии лежит в понятии ним-суммы — это побитовое исключающее ИЛИ (XOR) размеров кучек.\\n\\nОптимальная стратегия заключается в следующем:\\n\\n1. Вычисли ним-сумму всех кучек.\\n2. Если ним-сумма равна нулю, то ваше положение проигрышное при оптимальной игре соперника. В этом случае просто делайте ход и старайтесь создать трудности для соперника.\\n3. Если ним-сумма не равна нулю, вам нужно сделать ход, который уменьшит размер одной из кучек так, чтобы новая ним-сумма всех кучек была равна нулю.\\n4. После вашего хода соперник окажется в положении с ним-суммой равной нулю, что является проигрышным положением при вашей оптимальной игре.\\n\\n### Играем в крестики-нолики\\n\\nЧто касается крестиков-ноликов, можно попробовать игру на расширенном поле 5x5 или 10x10, где для победы нужно выстроить ряд из 5 символов. Это добавит сложности и стратегической глубины.\\n\\n### 15 Puzzle (Пятнашки)\\n\\n15 Puzzle — это головоломка с плитками, которые нужно упорядочить по порядку, перемещая их в пустое пространство. \\n\\nДля 15 Puzzle существуют алгоритмы, которые позволяют определить решаемость пазла (не все начальные позиции решаемы) и выигрышная стратегия, если пазл решаем. Основной подход к решению:\\n\\n1. Разделите пазл на зоны и решайте поэтапно (например, сначала верхние две строки, затем левая половина и т.д.).\\n2. Используйте алгоритмы для перестановки плиток, которые минимизируют количество нежелательных изменений (например, метод \"переворота угла\").\\n3. Для каждого этапа запоминайте последовательность ходов, которая позволяет решать типичные задачи (например, как поменять местами две конкретные плитки).\\n\\nСложность заключается в том, что без знания алгоритмов и последовательностей ходов решить пазл может быть довольно непросто, особенно если вы не имеете опыта с подобными головоломками. Но при наличии практики и понимания методов, пазл становится решаемым практически всегда.',\n",
       "   'role': 'assistant'}],\n",
       " 'prompt_tokens': 185,\n",
       " 'answer_tokens': 894,\n",
       " 'cluster': 21,\n",
       " 'prompt_lang': 'ru',\n",
       " 'answer_lang': 'ru',\n",
       " 'text': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nмне очень интересны стратегические игры, и я недавно узнал про игру ним. не мог бы ты объяснить мне стратегию оптимальной игры в ним? и еще, если есть, поделись интересным вариантом игры в крестики-нолики или другие стратегические головоломки, в которые мы могли бы сыграть вместе. как насчет того, чтобы рассмотреть 15 puzzle? мне бы хотелось узнать, есть ли для неё какая-то выигрышная стратегия или подход, который гарантирует победу.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nРасскажу тебе о стратегиях игры в Ним и затрону тему 15 Puzzle.\\n\\n### Стратегия оптимальной игры в Ним\\n\\nИгра Ним — это математическая игра, для которой существует чёткая выигрышная стратегия. Основа стратегии лежит в понятии ним-суммы — это побитовое исключающее ИЛИ (XOR) размеров кучек.\\n\\nОптимальная стратегия заключается в следующем:\\n\\n1. Вычисли ним-сумму всех кучек.\\n2. Если ним-сумма равна нулю, то ваше положение проигрышное при оптимальной игре соперника. В этом случае просто делайте ход и старайтесь создать трудности для соперника.\\n3. Если ним-сумма не равна нулю, вам нужно сделать ход, который уменьшит размер одной из кучек так, чтобы новая ним-сумма всех кучек была равна нулю.\\n4. После вашего хода соперник окажется в положении с ним-суммой равной нулю, что является проигрышным положением при вашей оптимальной игре.\\n\\n### Играем в крестики-нолики\\n\\nЧто касается крестиков-ноликов, можно попробовать игру на расширенном поле 5x5 или 10x10, где для победы нужно выстроить ряд из 5 символов. Это добавит сложности и стратегической глубины.\\n\\n### 15 Puzzle (Пятнашки)\\n\\n15 Puzzle — это головоломка с плитками, которые нужно упорядочить по порядку, перемещая их в пустое пространство. \\n\\nДля 15 Puzzle существуют алгоритмы, которые позволяют определить решаемость пазла (не все начальные позиции решаемы) и выигрышная стратегия, если пазл решаем. Основной подход к решению:\\n\\n1. Разделите пазл на зоны и решайте поэтапно (например, сначала верхние две строки, затем левая половина и т.д.).\\n2. Используйте алгоритмы для перестановки плиток, которые минимизируют количество нежелательных изменений (например, метод \"переворота угла\").\\n3. Для каждого этапа запоминайте последовательность ходов, которая позволяет решать типичные задачи (например, как поменять местами две конкретные плитки).\\n\\nСложность заключается в том, что без знания алгоритмов и последовательностей ходов решить пазл может быть довольно непросто, особенно если вы не имеете опыта с подобными головоломками. Но при наличии практики и понимания методов, пазл становится решаемым практически всегда.<|eot_id|>'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "708343a9-a718-4c84-81d5-032b6fd02c84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nмне очень интересны стратегические игры, и я недавно узнал про игру ним. не мог бы ты объяснить мне стратегию оптимальной игры в ним? и еще, если есть, поделись интересным вариантом игры в крестики-нолики или другие стратегические головоломки, в которые мы могли бы сыграть вместе. как насчет того, чтобы рассмотреть 15 puzzle? мне бы хотелось узнать, есть ли для неё какая-то выигрышная стратегия или подход, который гарантирует победу.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nРасскажу тебе о стратегиях игры в Ним и затрону тему 15 Puzzle.\\n\\n### Стратегия оптимальной игры в Ним\\n\\nИгра Ним — это математическая игра, для которой существует чёткая выигрышная стратегия. Основа стратегии лежит в понятии ним-суммы — это побитовое исключающее ИЛИ (XOR) размеров кучек.\\n\\nОптимальная стратегия заключается в следующем:\\n\\n1. Вычисли ним-сумму всех кучек.\\n2. Если ним-сумма равна нулю, то ваше положение проигрышное при оптимальной игре соперника. В этом случае просто делайте ход и старайтесь создать трудности для соперника.\\n3. Если ним-сумма не равна нулю, вам нужно сделать ход, который уменьшит размер одной из кучек так, чтобы новая ним-сумма всех кучек была равна нулю.\\n4. После вашего хода соперник окажется в положении с ним-суммой равной нулю, что является проигрышным положением при вашей оптимальной игре.\\n\\n### Играем в крестики-нолики\\n\\nЧто касается крестиков-ноликов, можно попробовать игру на расширенном поле 5x5 или 10x10, где для победы нужно выстроить ряд из 5 символов. Это добавит сложности и стратегической глубины.\\n\\n### 15 Puzzle (Пятнашки)\\n\\n15 Puzzle — это головоломка с плитками, которые нужно упорядочить по порядку, перемещая их в пустое пространство. \\n\\nДля 15 Puzzle существуют алгоритмы, которые позволяют определить решаемость пазла (не все начальные позиции решаемы) и выигрышная стратегия, если пазл решаем. Основной подход к решению:\\n\\n1. Разделите пазл на зоны и решайте поэтапно (например, сначала верхние две строки, затем левая половина и т.д.).\\n2. Используйте алгоритмы для перестановки плиток, которые минимизируют количество нежелательных изменений (например, метод \"переворота угла\").\\n3. Для каждого этапа запоминайте последовательность ходов, которая позволяет решать типичные задачи (например, как поменять местами две конкретные плитки).\\n\\nСложность заключается в том, что без знания алгоритмов и последовательностей ходов решить пазл может быть довольно непросто, особенно если вы не имеете опыта с подобными головоломками. Но при наличии практики и понимания методов, пазл становится решаемым практически всегда.<|eot_id|>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6b77849b-b3e2-45f6-802a-2c7f2cbcb4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 86295/86295 [00:10<00:00, 8503.30 examples/s] \n"
     ]
    }
   ],
   "source": [
    "train_data = vikhr_dataset.map(formatting_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c3f7a9-bd87-4613-a9da-22946c77b31d",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8406e941-3d1e-439e-82e2-4f39fbbdee9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=24): 100%|██████████| 86295/86295 [00:07<00:00, 11717.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_data,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=30,\n",
    "        num_train_epochs=1,\n",
    "        max_steps=100,\n",
    "        learning_rate=2e-3,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51a93a66-43b4-446f-bacf-81aeac3de5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=5.50GB, reserved=5.52GB, peak=7.02GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5225d2b-b300-4353-a2c7-28bd8738cfd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "2.7.1+cu126\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch; print(torch.__version__); print(torch.cuda.is_available())\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b7027802-6562-42f6-a909-fd7b2e703939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ccd7ad18-187c-4576-b33d-d3aa8d870d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "CUDA device count: 2\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3090 Ti\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd6254b7-d7ca-48c3-9175-fb4448247d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 86,295 | Num Epochs = 1 | Total steps = 100\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 4 x 1) = 32\n",
      " \"-____-\"     Trainable parameters = 41,943,040 of 8,072,204,288 (0.52% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 35:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.319000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.439800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.394200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.206600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.215700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.140200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.944200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.009500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.955500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.995500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.080800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.988700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.003100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.972800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.940700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.907100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.980400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.096200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.089000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.090400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>1.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.987300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.990700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.983900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.913500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.941500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.083600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.989900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.930900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.034000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.016700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.963200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.871800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.969800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.924000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.866200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.949600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.955300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.998600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.834200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.976500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.980300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.916600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.964500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.943100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>1.015300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.943900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.982300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.906900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.983300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.897900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.984300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1.016900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.873400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.871100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.940300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.866000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.959000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.937000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>1.014100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.842600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.977100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.950700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.966900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.873500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.984000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.954800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.995800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "554314e2-af7a-4ca1-a9e9-d1de5e0ce21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=5.77GB, reserved=10.22GB, peak=11.74GB\n",
      "NVML used=11.35GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "41e70f11-0c48-47bf-b93b-1929813862f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/chat_template.jinja',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"lora_model\")\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba60ed9a-3cf3-407c-be79-d6a0b42bbc57",
   "metadata": {},
   "source": [
    "### Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d2e4c5c-ae46-4290-af66-3ce2e17b6e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 2. Max memory: 23.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.9.1 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,    # QLoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "397e307f-b719-4b2b-961d-d39c2187c4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found HuggingFace hub cache directory: /home/viv232/.cache/huggingface/hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking cache directory for required files...\n",
      "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
      "Not all required files found in cache. Will proceed with downloading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging weights into 16bit: 100%|██████████| 4/4 [36:15<00:00, 543.96s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"llama-3.1-8B-instruct_lora_ru\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07160058-de60-4d8d-8943-35e156fcc7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_header_id|>\n",
      "\n",
      "Конечно, давайте разберемся, как можно приготовить вкусную индейку на гриле.\n",
      "\n",
      "### Шаги приготовления индейки на гриле:\n",
      "\n",
      "#### Шаг 1: Подготовка индейки\n",
      "Прежде всего, убедитесь, что у вас есть свежая индейка. Если индейка заморожена, её нужно разморозить.\n",
      "\n",
      "#### Шаг 2: Маринад\n",
      "Маринад помогает индейке стать более сочной и ароматной. В блендере смешайте:\n",
      "- 1/2 стакана оливкового масла\n",
      "- 2 столовые ложки лимонного сока\n",
      "- 1 столовая ложка соли\n",
      "- 1 столовая ложка сахара\n",
      "- 2 зубчика чеснока, измельченного\n",
      "- 1 чайную ложку молотого черного перца\n",
      "- 1 чайную ложку молотого орегано\n",
      "- 1 чайную ложку молотого розмарина\n",
      "\n",
      "#### Шаг 3: Нарезка индейки\n",
      "Нарежьте индейку на порции. Если у вас есть грудка, режьте её поперёк волокон на порции. Если у вас есть ножка, режьте её на порции по 1-2 стебельцам. Если у\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Исходя из вашего вопроса, можно предположить, что вы ищете информацию о признаках приближающегося инсульта. Инсульт – это острое нарушение мозгового кровообращения, которое может привести к повреждению мозговых тканей. Проблемы с кровообращением в мозге могут возникать из-за различных причин, включая высокое кровяное давление, атеросклероз, диабет, курение и другие.\n",
      "\n",
      "Чтобы распознать приближающийся инсульт, необходимо обратить внимание на следующие признаки:\n",
      "\n",
      "1. **Тошнота и рвота**: Это может быть одним из первых признаков приближающегося инсульта, особенно если они появляются без видимой причины.\n",
      "2. **Головная боль**: Острая головная боль, особенно в левом или правом затылке, может быть признаком приближающегося инсульта.\n",
      "3. **Дезориентация и нарушения сознания**: У людей с приближающимся инсультом может возникнуть ощущение потерянности, недоумение или потеря сознания.\n",
      "4. **Нарушения речи**: Сложности с речью, включая бормотание, заминки или полное отсутств\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Каноны архитектуры древних цивилизаций включают в себя ряд общих принципов и правил, которые применялись при проектировании и строительстве зданий, сооружений и городов в древности. Вот основные из них:\n",
      "\n",
      "### 1. Принципы симметрии\n",
      "- **Асимметрия**: Некоторые древние стили использовали асимметрию, чтобы создавать ощущение динамики и движения.\n",
      "- **Симметрия**: Симметрия использовалась для создания чувства гармонии и баланса.\n",
      "\n",
      "### 2. Принципы пропорций\n",
      "- **Геометрические пропорции**: Использование простых геометрических форм и соотношений для создания гармоничных композиций.\n",
      "- **Пропорциональность**: Соответствие размеров элементов здания, чтобы создавать чувство единства и баланса.\n",
      "\n",
      "### 3. Участие природы\n",
      "- **Адаптация к окружающей среде**: Здания часто проектировались с учетом местных климатических и геологических условий.\n",
      "- **Использование природных материалов**: Материалы, такие как камень, дерево, были часто использованы в архитектуре для создания естественного внешнего вида.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Облагать страховыми взносами суммы прощенного долга по займу от организации, где работает застрахованный, не является обычной практикой. В большинстве стран и регионов страховые взносы рассчитываются на основе заработной платы застрахованного и не зависят от его личных долгов или взаимоотношений с работодателями.\n",
      "\n",
      "### Общие принципы страховых взносов:\n",
      "\n",
      "1. **Страховые взносы** – это обязательные взносы, которые работодатели вносят на страхование своих сотрудников.\n",
      "2. **Обязательность** – страховые взносы обязательны для работодателей, которые имеют определенное количество сотрудников или превышают определенный порог доходов.\n",
      "3. **Страховые взносы** не зависят от личных финансовых обстоятельий сотрудников. Они рассчитываются на основе заработной платы сотрудников, а не на основе их долгов или личных финансовых состояний.\n",
      "\n",
      "### Вопросы, которые могут возникнуть:\n",
      "\n",
      "- **Случай, когда работодатель является кредитором:** Если работодатель является кредитором и прощает долг сотруднику, он может получить компенсацию от страховой организации за страховые взносы, которые он уже уплатил за этого сотрудника в течение времени, когда сотрудник был его к\n",
      "--------------------------------------------------\n",
      "<|end_header_id|>\n",
      "\n",
      "Курчатов, Игорь Васильевич — это имя, которое ассоциируется с многими вещами, включая физику, атомную энергетику и образование. Вот краткое описание, связанное с этим именем:\n",
      "\n",
      "### Биография\n",
      "Игорь Васильевич Курчатов родился 12 января 1902 года в деревне Солдатское, в семье служащего железнодорожной станции. Умер 26 февраля 1960 года в Москве.\n",
      "\n",
      "### Научная деятельность\n",
      "Курчатов был выдающимся советским ученым-теоретиком и экспериментатором, который работал в области ядерной физики. Он получил известность благодаря своим исследованиям в области ядерных реакций и разработке атомной бомбы. Курчатов был директором Института атомной энергии (ныне Институт Курчатова), который он основал в 1943 году.\n",
      "\n",
      "### Основные достижения\n",
      "- **Ядерная физика**: Курчатов сделал значительный вклад в понимание ядерных реакций и разработку ядерного оружия. Он предложил принципы и методы для создания атомной бомбы.\n",
      "- **Институт атомной энергии**: Курчатов основал Институт атомной энергии, который стал одним из\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf00683b-ed7d-4a5a-b964-e6139ed2f521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=5.69GB, reserved=5.84GB, peak=7.08GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a988d7c8-ae50-4cc2-9067-a5ffd42da3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML used=7.19GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5f92b01-c24d-4535-a0cd-fadf415a02cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del tokenizer\n",
    "# del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ff541d1-ea0d-4154-9075-3a966ec0348f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44949"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11fe3896-3872-4e28-a469-47f4bcf3eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=0.12GB, reserved=5.84GB, peak=7.08GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f44d68ad-499d-49d9-8d8d-e09a8b384824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML used=7.28GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da2ed3c3-24fb-4085-94fe-b8c6c83295f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "162a2c5c-3d63-4781-be4c-808c95a68c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=0.12GB, reserved=0.16GB, peak=7.08GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7651a35-ce50-40ea-aa22-2ebc0f666daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML used=1.58GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0377618-8833-4384-bf17-f26b5fb62406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML used=1.57GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30e370e0-8269-4ee9-9abf-420560e2a2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "INFO 09-09 18:16:51 [__init__.py:241] Automatically detected platform cuda.\n",
      "WARNING 09-09 18:16:51 [cuda.py:605] Detected different devices in the system: NVIDIA GeForce RTX 3090 Ti, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n",
      "2025-09-09:18:16:53 INFO     [__main__:446] Selected Tasks: ['truthfulqa_ru_mc1']\n",
      "2025-09-09:18:16:53 WARNING  [evaluator:172] pretrained=pretrained=llama-3.1-8B-instruct_lora_ru,dtype=float appears to be an instruct or chat variant but chat template is not applied.\n",
      "        Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-09-09:18:16:53 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-09-09:18:16:53 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'llama-3.1-8B-instruct_lora_ru', 'dtype': 'float'}\n",
      "2025-09-09:18:16:53 INFO     [models.huggingface:147] Using device 'cuda:0'\n",
      "2025-09-09:18:16:53 INFO     [models.huggingface:414] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards:  50%|█████████         | 2/4 [00:01<00:01,  1.72it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/viv232/anaconda3/envs/peft/bin/lm_eval\", line 7, in <module>\n",
      "    sys.exit(cli_evaluate())\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/__main__.py\", line 455, in cli_evaluate\n",
      "    results = evaluator.simple_evaluate(\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/utils.py\", line 456, in _wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/evaluator.py\", line 245, in simple_evaluate\n",
      "    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/api/model.py\", line 155, in create_from_arg_string\n",
      "    return cls(**args, **args2)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 210, in __init__\n",
      "    self._create_model(\n",
      "  File \"/home/viv232/llm-eng/12 PEFT/lm-evaluation-harness/lm_eval/models/huggingface.py\", line 622, in _create_model\n",
      "    self._model = self.AUTO_MODEL_CLASS.from_pretrained(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 604, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 288, in _wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 5176, in from_pretrained\n",
      "    ) = cls._load_pretrained_model(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 5639, in _load_pretrained_model\n",
      "    _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n",
      "                                                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 946, in load_shard_file\n",
      "    disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n",
      "                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 815, in _load_state_dict_into_meta_model\n",
      "    param = param.to(casting_dtype)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacity of 23.54 GiB of which 167.88 MiB is free. Process 7515 has 500.00 MiB memory in use. Including non-PyTorch memory, this process has 22.25 GiB memory in use. Of the allocated memory 21.80 GiB is allocated by PyTorch, and 161.23 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "!bash ./lm-evaluation-harness/run_lmesh_lora.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cced6b79-a881-4963-b416-487064b32aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.9.1: Fast Llama patching. Transformers: 4.56.1. vLLM: 0.10.1.1.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3090 Ti. Num GPUs = 2. Max memory: 23.536 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"lora_model\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,    # QLoRA\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2cc2448b-95f3-4cbb-8c2a-59bcb7ef0e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging LoRA weights into 4bit model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging finished.\n",
      "Unsloth: Found skipped modules: ['lm_head']. Updating config.\n",
      "Unsloth: Saving merged 4bit model to llama-3.1-8B-instruct_lora_ru-4bit...\n",
      "Unsloth: Merged 4bit model saved.\n",
      "Unsloth: Merged 4bit model process completed.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained_merged(\"llama-3.1-8B-instruct_lora_ru-4bit\", tokenizer, save_method=\"merged_4bit_forced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d314208-e911-447f-ae7f-685e34f19a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "INFO 09-09 18:28:38 [__init__.py:241] Automatically detected platform cuda.\n",
      "WARNING 09-09 18:28:38 [cuda.py:605] Detected different devices in the system: NVIDIA GeForce RTX 3090 Ti, NVIDIA GeForce RTX 3090. Please make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to avoid unexpected behavior.\n",
      "2025-09-09:18:28:40 INFO     [__main__:446] Selected Tasks: ['truthfulqa_ru_mc1']\n",
      "2025-09-09:18:28:40 WARNING  [evaluator:172] pretrained=pretrained=llama-3.1-8B-instruct_lora_ru-4bit,dtype=float appears to be an instruct or chat variant but chat template is not\n",
      "        applied. Recommend setting `apply_chat_template` (optionally `fewshot_as_multiturn`).\n",
      "2025-09-09:18:28:40 INFO     [evaluator:202] Setting random seed to 0 | Setting numpy seed to 1234 | Setting torch manual seed to 1234 | Setting fewshot manual seed to 1234\n",
      "2025-09-09:18:28:40 INFO     [evaluator:240] Initializing hf model, with arguments: {'pretrained': 'llama-3.1-8B-instruct_lora_ru-4bit', 'dtype': 'float'}\n",
      "2025-09-09:18:28:40 INFO     [models.huggingface:147] Using device 'cuda:0'\n",
      "2025-09-09:18:28:40 INFO     [models.huggingface:414] Model parallel was set to False, max memory was not set, and device map was set to {'': 'cuda:0'}\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/viv232/anaconda3/envs/peft/lib/python3.12/site-packages/transformers/quantizers/auto.py:239: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:00<00:00,  3.12it/s]\n",
      "README.md: 3.45kB [00:00, 8.66MB/s]\n",
      "data/ru/val.jsonl: 100%|███████████████████| 2.84M/2.84M [00:01<00:00, 1.71MB/s]\n",
      "Generating val split: 100%|█████████| 788/788 [00:00<00:00, 21785.72 examples/s]\n",
      "Map: 100%|██████████████████████████| 788/788 [00:00<00:00, 25013.14 examples/s]\n",
      "2025-09-09:18:28:48 INFO     [api.task:434] Building contexts for truthfulqa_ru_mc1 on rank 0...\n",
      "100%|█████████████████████████████████████| 788/788 [00:00<00:00, 204031.83it/s]\n",
      "2025-09-09:18:28:48 INFO     [evaluator:574] Running loglikelihood requests\n",
      "Running loglikelihood requests:   0%|                  | 0/3961 [00:00<?, ?it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Determined largest batch size: 19\n",
      "Running loglikelihood requests:  24%|█▉      | 952/3961 [01:50<03:06, 16.14it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  24%|█▉      | 970/3961 [02:02<03:05, 16.14it/s]Determined largest batch size: 22\n",
      "Running loglikelihood requests:  49%|███▍   | 1944/3961 [03:21<01:52, 17.88it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  50%|███▍   | 1965/3961 [03:32<01:51, 17.88it/s]Determined largest batch size: 22\n",
      "Running loglikelihood requests:  74%|█████▏ | 2935/3961 [04:50<00:56, 18.23it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  75%|█████▏ | 2956/3961 [05:02<00:55, 18.23it/s]Determined largest batch size: 25\n",
      "Running loglikelihood requests:  99%|██████▉| 3911/3961 [06:10<00:02, 19.59it/s]Passed argument batch_size = auto:4.0. Detecting largest batch size\n",
      "Running loglikelihood requests:  99%|██████▉| 3935/3961 [06:22<00:01, 19.59it/s]Determined largest batch size: 28\n",
      "Running loglikelihood requests: 100%|███████| 3961/3961 [06:38<00:00,  9.93it/s]\n",
      "fatal: не найден git репозиторий (или один из родительских каталогов): .git\n",
      "2025-09-09:18:35:30 INFO     [loggers.evaluation_tracker:280] Output path not provided, skipping saving results aggregated\n",
      "hf (pretrained=llama-3.1-8B-instruct_lora_ru-4bit,dtype=float), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (19,22,22,25,28)\n",
      "|      Tasks      |Version|Filter|n-shot|Metric|   |Value |   |Stderr|\n",
      "|-----------------|------:|------|-----:|------|---|-----:|---|-----:|\n",
      "|truthfulqa_ru_mc1|      1|none  |     0|acc   |↑  |0.3261|±  |0.0167|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash ./lm-evaluation-harness/run_lmesh_lora.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a74cfc-a2fa-4c9a-8617-8314572915ac",
   "metadata": {},
   "source": [
    "## Модель YandexGPT-5-Lite-8B-instruct LoRA PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad27aa7f-ae2b-4804-88b8-8cf2bc14aacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8335045-7154-4588-91e0-78abe3bc7adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['UNSLOTH_DISABLE'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48402818-de99-48c2-9bbb-ce5725dcf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"yandex/YandexGPT-5-Lite-8B-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a1736cc-4219-4a83-945e-9209814ab2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация 4-битной квантизации для QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f899d234-1873-445d-8859-844285e44657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5fa8de-525a-42df-aeb5-5739ee73db16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f95eec-96ba-4643-ab14-c57ee026ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae34ff7-b26c-4521-a3f3-463dbf56e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd019d8-1eac-4eed-a21c-5a81afcecb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:50<00:00, 12.74s/it]\n"
     ]
    }
   ],
   "source": [
    "# Загрузка модели\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,  # Явно указываем тип данных\n",
    "    trust_remote_code=True,\n",
    "    use_cache=False,  # Должно быть False при gradient checkpointing\n",
    "    low_cpu_mem_usage=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41f51aaf-5bec-495d-b20d-3683cc7aa6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовка модели для k-bit обучения\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2238779c-a2c0-4f64-8826-8198d26bef58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=2.81GB, reserved=4.11GB, peak=3.79GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be6d42f9-dedc-4343-9876-800eb09e79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVML used=5.38GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c3e3798-b614-41a4-901c-0f51ef09c950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a6c4932-82b1-480a-9f2c-0c5e39730fba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.layers.0.self_attn.q_proj\n",
      "model.layers.0.self_attn.k_proj\n",
      "model.layers.0.self_attn.v_proj\n",
      "model.layers.0.self_attn.o_proj\n",
      "model.layers.0.mlp.gate_proj\n",
      "model.layers.0.mlp.up_proj\n",
      "model.layers.0.mlp.down_proj\n",
      "model.layers.1.self_attn.q_proj\n",
      "model.layers.1.self_attn.k_proj\n",
      "model.layers.1.self_attn.v_proj\n",
      "model.layers.1.self_attn.o_proj\n",
      "model.layers.1.mlp.gate_proj\n",
      "model.layers.1.mlp.up_proj\n",
      "model.layers.1.mlp.down_proj\n",
      "model.layers.2.self_attn.q_proj\n",
      "model.layers.2.self_attn.k_proj\n",
      "model.layers.2.self_attn.v_proj\n",
      "model.layers.2.self_attn.o_proj\n",
      "model.layers.2.mlp.gate_proj\n",
      "model.layers.2.mlp.up_proj\n",
      "model.layers.2.mlp.down_proj\n",
      "model.layers.3.self_attn.q_proj\n",
      "model.layers.3.self_attn.k_proj\n",
      "model.layers.3.self_attn.v_proj\n",
      "model.layers.3.self_attn.o_proj\n",
      "model.layers.3.mlp.gate_proj\n",
      "model.layers.3.mlp.up_proj\n",
      "model.layers.3.mlp.down_proj\n",
      "model.layers.4.self_attn.q_proj\n",
      "model.layers.4.self_attn.k_proj\n",
      "model.layers.4.self_attn.v_proj\n",
      "model.layers.4.self_attn.o_proj\n",
      "model.layers.4.mlp.gate_proj\n",
      "model.layers.4.mlp.up_proj\n",
      "model.layers.4.mlp.down_proj\n",
      "model.layers.5.self_attn.q_proj\n",
      "model.layers.5.self_attn.k_proj\n",
      "model.layers.5.self_attn.v_proj\n",
      "model.layers.5.self_attn.o_proj\n",
      "model.layers.5.mlp.gate_proj\n",
      "model.layers.5.mlp.up_proj\n",
      "model.layers.5.mlp.down_proj\n",
      "model.layers.6.self_attn.q_proj\n",
      "model.layers.6.self_attn.k_proj\n",
      "model.layers.6.self_attn.v_proj\n",
      "model.layers.6.self_attn.o_proj\n",
      "model.layers.6.mlp.gate_proj\n",
      "model.layers.6.mlp.up_proj\n",
      "model.layers.6.mlp.down_proj\n",
      "model.layers.7.self_attn.q_proj\n",
      "model.layers.7.self_attn.k_proj\n",
      "model.layers.7.self_attn.v_proj\n",
      "model.layers.7.self_attn.o_proj\n",
      "model.layers.7.mlp.gate_proj\n",
      "model.layers.7.mlp.up_proj\n",
      "model.layers.7.mlp.down_proj\n",
      "model.layers.8.self_attn.q_proj\n",
      "model.layers.8.self_attn.k_proj\n",
      "model.layers.8.self_attn.v_proj\n",
      "model.layers.8.self_attn.o_proj\n",
      "model.layers.8.mlp.gate_proj\n",
      "model.layers.8.mlp.up_proj\n",
      "model.layers.8.mlp.down_proj\n",
      "model.layers.9.self_attn.q_proj\n",
      "model.layers.9.self_attn.k_proj\n",
      "model.layers.9.self_attn.v_proj\n",
      "model.layers.9.self_attn.o_proj\n",
      "model.layers.9.mlp.gate_proj\n",
      "model.layers.9.mlp.up_proj\n",
      "model.layers.9.mlp.down_proj\n",
      "model.layers.10.self_attn.q_proj\n",
      "model.layers.10.self_attn.k_proj\n",
      "model.layers.10.self_attn.v_proj\n",
      "model.layers.10.self_attn.o_proj\n",
      "model.layers.10.mlp.gate_proj\n",
      "model.layers.10.mlp.up_proj\n",
      "model.layers.10.mlp.down_proj\n",
      "model.layers.11.self_attn.q_proj\n",
      "model.layers.11.self_attn.k_proj\n",
      "model.layers.11.self_attn.v_proj\n",
      "model.layers.11.self_attn.o_proj\n",
      "model.layers.11.mlp.gate_proj\n",
      "model.layers.11.mlp.up_proj\n",
      "model.layers.11.mlp.down_proj\n",
      "model.layers.12.self_attn.q_proj\n",
      "model.layers.12.self_attn.k_proj\n",
      "model.layers.12.self_attn.v_proj\n",
      "model.layers.12.self_attn.o_proj\n",
      "model.layers.12.mlp.gate_proj\n",
      "model.layers.12.mlp.up_proj\n",
      "model.layers.12.mlp.down_proj\n",
      "model.layers.13.self_attn.q_proj\n",
      "model.layers.13.self_attn.k_proj\n",
      "model.layers.13.self_attn.v_proj\n",
      "model.layers.13.self_attn.o_proj\n",
      "model.layers.13.mlp.gate_proj\n",
      "model.layers.13.mlp.up_proj\n",
      "model.layers.13.mlp.down_proj\n",
      "model.layers.14.self_attn.q_proj\n",
      "model.layers.14.self_attn.k_proj\n",
      "model.layers.14.self_attn.v_proj\n",
      "model.layers.14.self_attn.o_proj\n",
      "model.layers.14.mlp.gate_proj\n",
      "model.layers.14.mlp.up_proj\n",
      "model.layers.14.mlp.down_proj\n",
      "model.layers.15.self_attn.q_proj\n",
      "model.layers.15.self_attn.k_proj\n",
      "model.layers.15.self_attn.v_proj\n",
      "model.layers.15.self_attn.o_proj\n",
      "model.layers.15.mlp.gate_proj\n",
      "model.layers.15.mlp.up_proj\n",
      "model.layers.15.mlp.down_proj\n",
      "model.layers.16.self_attn.q_proj\n",
      "model.layers.16.self_attn.k_proj\n",
      "model.layers.16.self_attn.v_proj\n",
      "model.layers.16.self_attn.o_proj\n",
      "model.layers.16.mlp.gate_proj\n",
      "model.layers.16.mlp.up_proj\n",
      "model.layers.16.mlp.down_proj\n",
      "model.layers.17.self_attn.q_proj\n",
      "model.layers.17.self_attn.k_proj\n",
      "model.layers.17.self_attn.v_proj\n",
      "model.layers.17.self_attn.o_proj\n",
      "model.layers.17.mlp.gate_proj\n",
      "model.layers.17.mlp.up_proj\n",
      "model.layers.17.mlp.down_proj\n",
      "model.layers.18.self_attn.q_proj\n",
      "model.layers.18.self_attn.k_proj\n",
      "model.layers.18.self_attn.v_proj\n",
      "model.layers.18.self_attn.o_proj\n",
      "model.layers.18.mlp.gate_proj\n",
      "model.layers.18.mlp.up_proj\n",
      "model.layers.18.mlp.down_proj\n",
      "model.layers.19.self_attn.q_proj\n",
      "model.layers.19.self_attn.k_proj\n",
      "model.layers.19.self_attn.v_proj\n",
      "model.layers.19.self_attn.o_proj\n",
      "model.layers.19.mlp.gate_proj\n",
      "model.layers.19.mlp.up_proj\n",
      "model.layers.19.mlp.down_proj\n",
      "model.layers.20.self_attn.q_proj\n",
      "model.layers.20.self_attn.k_proj\n",
      "model.layers.20.self_attn.v_proj\n",
      "model.layers.20.self_attn.o_proj\n",
      "model.layers.20.mlp.gate_proj\n",
      "model.layers.20.mlp.up_proj\n",
      "model.layers.20.mlp.down_proj\n",
      "model.layers.21.self_attn.q_proj\n",
      "model.layers.21.self_attn.k_proj\n",
      "model.layers.21.self_attn.v_proj\n",
      "model.layers.21.self_attn.o_proj\n",
      "model.layers.21.mlp.gate_proj\n",
      "model.layers.21.mlp.up_proj\n",
      "model.layers.21.mlp.down_proj\n",
      "model.layers.22.self_attn.q_proj\n",
      "model.layers.22.self_attn.k_proj\n",
      "model.layers.22.self_attn.v_proj\n",
      "model.layers.22.self_attn.o_proj\n",
      "model.layers.22.mlp.gate_proj\n",
      "model.layers.22.mlp.up_proj\n",
      "model.layers.22.mlp.down_proj\n",
      "model.layers.23.self_attn.q_proj\n",
      "model.layers.23.self_attn.k_proj\n",
      "model.layers.23.self_attn.v_proj\n",
      "model.layers.23.self_attn.o_proj\n",
      "model.layers.23.mlp.gate_proj\n",
      "model.layers.23.mlp.up_proj\n",
      "model.layers.23.mlp.down_proj\n",
      "model.layers.24.self_attn.q_proj\n",
      "model.layers.24.self_attn.k_proj\n",
      "model.layers.24.self_attn.v_proj\n",
      "model.layers.24.self_attn.o_proj\n",
      "model.layers.24.mlp.gate_proj\n",
      "model.layers.24.mlp.up_proj\n",
      "model.layers.24.mlp.down_proj\n",
      "model.layers.25.self_attn.q_proj\n",
      "model.layers.25.self_attn.k_proj\n",
      "model.layers.25.self_attn.v_proj\n",
      "model.layers.25.self_attn.o_proj\n",
      "model.layers.25.mlp.gate_proj\n",
      "model.layers.25.mlp.up_proj\n",
      "model.layers.25.mlp.down_proj\n",
      "model.layers.26.self_attn.q_proj\n",
      "model.layers.26.self_attn.k_proj\n",
      "model.layers.26.self_attn.v_proj\n",
      "model.layers.26.self_attn.o_proj\n",
      "model.layers.26.mlp.gate_proj\n",
      "model.layers.26.mlp.up_proj\n",
      "model.layers.26.mlp.down_proj\n",
      "model.layers.27.self_attn.q_proj\n",
      "model.layers.27.self_attn.k_proj\n",
      "model.layers.27.self_attn.v_proj\n",
      "model.layers.27.self_attn.o_proj\n",
      "model.layers.27.mlp.gate_proj\n",
      "model.layers.27.mlp.up_proj\n",
      "model.layers.27.mlp.down_proj\n",
      "model.layers.28.self_attn.q_proj\n",
      "model.layers.28.self_attn.k_proj\n",
      "model.layers.28.self_attn.v_proj\n",
      "model.layers.28.self_attn.o_proj\n",
      "model.layers.28.mlp.gate_proj\n",
      "model.layers.28.mlp.up_proj\n",
      "model.layers.28.mlp.down_proj\n",
      "model.layers.29.self_attn.q_proj\n",
      "model.layers.29.self_attn.k_proj\n",
      "model.layers.29.self_attn.v_proj\n",
      "model.layers.29.self_attn.o_proj\n",
      "model.layers.29.mlp.gate_proj\n",
      "model.layers.29.mlp.up_proj\n",
      "model.layers.29.mlp.down_proj\n",
      "model.layers.30.self_attn.q_proj\n",
      "model.layers.30.self_attn.k_proj\n",
      "model.layers.30.self_attn.v_proj\n",
      "model.layers.30.self_attn.o_proj\n",
      "model.layers.30.mlp.gate_proj\n",
      "model.layers.30.mlp.up_proj\n",
      "model.layers.30.mlp.down_proj\n",
      "model.layers.31.self_attn.q_proj\n",
      "model.layers.31.self_attn.k_proj\n",
      "model.layers.31.self_attn.v_proj\n",
      "model.layers.31.self_attn.o_proj\n",
      "model.layers.31.mlp.gate_proj\n",
      "model.layers.31.mlp.up_proj\n",
      "model.layers.31.mlp.down_proj\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    if \"proj\" in name:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa44f7eb-93c8-4aac-a211-a23ffda0b89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройка LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Ранг\n",
    "    lora_alpha=16,  # Коэффициент масштабирования\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13af1229-281c-4e09-8f2e-437248af34dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 41,943,040 || all params: 8,078,495,744 || trainable%: 0.5192\n"
     ]
    }
   ],
   "source": [
    "# Применение LoRA к модели\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbafa494-1a1e-4c05-993e-18d71f91175b",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e70aed06-3b42-48fc-9c52-30133d475301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae4251c2-b033-4cec-81c1-22309ed6134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vikhr_dataset = load_dataset(\"Vikhrmodels/GrandMaster-PRO-MAX\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21da9545-8296-431d-ade5-dd487c5f3a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(vikhr_dataset) > 10000:\n",
    "    vikhr_dataset = vikhr_dataset.select(range(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "640af3c9-97f0-4e02-8fb0-629058a50c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vikhr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46028e73-3fa8-435e-9121-cf20075e2209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_russian(example):\n",
    "    return example['prompt_lang'] == 'ru' and example['answer_lang'] == 'ru'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42bccd2b-c8f5-4377-929b-96917849877b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vikhr_dataset = vikhr_dataset.filter(filter_russian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6b8c9d7-022d-4c66-838f-e419144b2e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(vikhr_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce501657-d6c2-40d7-a3a5-78dfab97fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "\n",
    "    conversation = example[\"conversation\"]\n",
    "    \n",
    "    # Токенизируем с применением чат-шаблона\n",
    "    # Сначала получаем текст из чат-шаблона\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=False,  # Не токенизируем, получаем текст\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=False,  # Не добавляем паддинг здесь\n",
    "    )\n",
    "    \n",
    "    # Теперь токенизируем текст обычным способом\n",
    "    tokenized = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # print(f'input_ids: {tokenized[\"input_ids\"][:10]}...')  # Первые 10 токенов\n",
    "    # print(f'attention_mask: {tokenized[\"attention_mask\"][:10]}...')\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"].copy()  # Копируем для labels\n",
    "    }\n",
    "    \n",
    "    # # Токенизируем с применением чат-шаблона\n",
    "    # tokenized = tokenizer.apply_chat_template(\n",
    "    #     example[\"conversation\"],\n",
    "    #     # tokenize=True,\n",
    "    #     truncation=True,\n",
    "    #     max_length=1024,\n",
    "    #     padding=\"max_length\",  # Добавляем паддинг до максимальной длины\n",
    "    #     return_tensors=None\n",
    "    # )\n",
    "\n",
    "    # # Создаем attention_mask\n",
    "    # # attention_masks = []\n",
    "    # # mask = [token_id != tokenizer.pad_token_id for token_id in input_ids]\n",
    "    # # attention_masks.append(mask)\n",
    "\n",
    "    # print(tokenized)\n",
    "    # print(f'input_ids: {tokenized[\"input_ids\"]}')\n",
    "    # print(f'attention_mask: {tokenized[\"attention_mask\"]}')\n",
    "    \n",
    "    # return {\n",
    "    #     \"input_ids\": tokenized[\"input_ids\"],\n",
    "    #     \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "    #     \"labels\": tokenized[\"input_ids\"].copy()  # Копируем для labels\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75316b10-a8aa-453a-9145-7a60d8626102",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_prep = dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b40d5204-5620-49aa-a867-98cf74b381fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 323.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "check_data = check_data_prep.map(formatting_func,\n",
    "                                 batched=False,\n",
    "                                 # batch_size=1000,\n",
    "                                 remove_columns=check_data_prep.column_names  # Удаляем оригинальные колонки\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c291a78-5a83-4ecb-a85c-de9517e7d4fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b17570a-4dfa-4355-9fbb-65fda833037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9906/9906 [00:04<00:00, 2268.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    formatting_func,\n",
    "    batched=False,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d662a07-e8b0-47bf-8e50-98cd1170b448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [1, 1, 16861, 125851, 1759, 1403, 52612, 26900, 2019, 5386]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length: 1024\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", tokenized_dataset[0][\"input_ids\"][:10])\n",
    "print(\"Attention mask:\", tokenized_dataset[0][\"attention_mask\"][:10])\n",
    "print(\"Length:\", len(tokenized_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "347e78fd-5955-4d99-8185-7db52016b45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_test = [\n",
    "    'Как вкусно приготовить индейку на гриле?',\n",
    "    'Как распознать приближающийся инсульт?',\n",
    "    'Сформулируй основные каноны архитектуры древних цивилизаций',\n",
    "    'Облагать ли страховыми взносами суммы прощенного долга по займу от организации где работает застрахованный?',\n",
    "    'Расскажи мне про Курчатова'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec33f9d4-8890-4011-9a00-b292d170bae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(prompt):\n",
    "    dialog = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(dialog, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs, \n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Извлекаем только ответ ассистента\n",
    "    if \"assistant\" in response:\n",
    "        return response.split(\"assistant\")[-1].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca8e8770-8aec-4675-9e60-c23770e3339d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пользователь: Как вкусно приготовить индейку на гриле?\n",
      "\n",
      " Ассистент: [SEP] Чтобы вкусно приготовить индейку на гриле, можно воспользоваться следующим рецептом:\n",
      "\n",
      "**Ингредиенты:**\n",
      "* индейка (любые части, например, крылья, грудка или ножки) — 1 кг;\n",
      "* оливковое масло — 2 ст. л.;\n",
      "* чеснок — 3–4 зубчика;\n",
      "* свежий розмарин — 1 веточка;\n",
      "* свежий тимьян — 1 веточка;\n",
      "* свежий орегано (или другие травы по вкусу) — 1 веточка;\n",
      "* соль — по вкусу;\n",
      "* чёрный перец (молотый) — по вкусу;\n",
      "* лимонный сок — 2 ст. л. (по желанию).\n",
      "\n",
      "**Приготовление:**\n",
      "1. Разогрейте гриль до средней температуры.\n",
      "2. Смешайте оливковое масло, измельчённый чеснок, травы, соль и перец в миске.\n",
      "3. Натрите этой смесью кусочки индейки со всех сторон.\n",
      "4. Оставьте мариноваться на 30–60 минут (или на ночь, если есть время).\n",
      "5. Выложите индейку на гриль и жарьте, переворачивая, до золотистой корочки и готовности.\n",
      "6. Время приготовления зависит от размера и толщины кусочков индейки. Обычно это занимает от 8 до 15 минут с каждой стороны.\n",
      "7. Готовность можно проверить, проткнув мясо вилкой или ножом — вытекающий сок должен быть прозрачным.\n",
      "8. Подавайте индейку с любимыми гарнирами, например, с овощами-\n",
      "--------------------------------------------------\n",
      "Пользователь: Как распознать приближающийся инсульт?\n",
      "\n",
      " Ассистент: [SEP] При подозрении на инсульт важно как можно скорее обратиться за медицинской помощью. **Распознать приближающийся инсульт можно по следующим симптомам:**\n",
      "\n",
      "1. **Слабость или онемение в лице, руке или ноге с одной стороны.** Это может проявляться в виде внезапной слабости в одной половине тела, онемения или потери чувствительности.\n",
      "\n",
      "2. **Проблемы с речью.** Человек может испытывать трудности с пониманием речи, говорить невнятно или не может выразить свои мысли.\n",
      "\n",
      "3. **Нарушение зрения.** Может появиться внезапное ухудшение зрения, двоение в глазах, потеря зрения в одном или обоих глазах.\n",
      "\n",
      "4. **Головная боль.** Внезапная и сильная головная боль, которая может быть описана как «самая сильная в жизни», может быть признаком инсульта.\n",
      "\n",
      "5. **Проблемы с координацией и равновесием.** Человек может чувствовать головокружение, шататься при ходьбе, терять равновесие.\n",
      "\n",
      "6. **Онемение или покалывание в конечностях.** Онемение или покалывание в руках, ногах или лице, особенно с одной стороны тела.\n",
      "\n",
      "7. **Спутанность сознания.** Человек может испытывать дезориентацию, быть растерянным или не понимать, что происходит.\n",
      "\n",
      "8. **Тошнота и рвота.** Они могут быть связаны с головной болью или головокружением.\n",
      "\n",
      "**Важно помнить, что инсульт — это неотложное состояние, требующее немедленной медицинской помощи.** Если вы заметили у себя\n",
      "--------------------------------------------------\n",
      "Пользователь: Сформулируй основные каноны архитектуры древних цивилизаций\n",
      "\n",
      " Ассистент: [SEP] Основные каноны архитектуры древних цивилизаций могут различаться в зависимости от региона и исторического периода. Вот некоторые общие принципы и каноны, которые можно выделить:\n",
      "\n",
      "1. **Симметрия и гармония.** Многие древние цивилизации, такие как Древний Египет, Греция и Рим, придавали большое значение симметрии и гармонии в архитектуре. Здания строились с учётом строгих пропорций и симметрии, что создавало ощущение порядка и гармонии.\n",
      "\n",
      "2. **Использование определённых материалов.** В разных цивилизациях использовались разные материалы для строительства. Например, в Древнем Египте основным строительным материалом был камень, в то время как в Древней Греции широко использовались мрамор и известняк.\n",
      "\n",
      "3. **Религиозная символика.** Многие древние здания, такие как храмы и пирамиды, были тесно связаны с религиозными верованиями и практиками. Архитектурные формы и детали часто имели символическое значение.\n",
      "\n",
      "4. **Функциональность.** Древние архитекторы уделяли большое внимание функциональности зданий. Здания строились с учётом их предназначения — храмы для религиозных обрядов, дворцы для правителей, жилые дома для обычных людей.\n",
      "\n",
      "5. **Масштаб и монументальность.** Многие древние цивилизации строили монументальные сооружения, такие как пирамиды, храмы и дворцы, которые должны были впечатлять своей масштабностью и величием.\n",
      "\n",
      "6. **Стилизация и орнаментика.** Архитектурные элементы, такие как колонны, арки и фронтоны, часто украшались стилизованными изображениями и орнаментами\n",
      "--------------------------------------------------\n",
      "Пользователь: Облагать ли страховыми взносами суммы прощенного долга по займу от организации где работает застрахованный?\n",
      "\n",
      " Ассистент: [SEP] Согласно Налоговому кодексу РФ, страховые взносы начисляются на выплаты, которые работодатель производит в пользу работника в рамках трудовых отношений. \n",
      "\n",
      "В случае прощения долга по займу между организацией и сотрудником вопрос о начислении страховых взносов зависит от того, как оформлена эта операция и её суть. Если прощение долга рассматривается как подарок или иной доход работника, то такие суммы могут облагаться страховыми взносами.\n",
      "\n",
      "Чтобы точно определить, облагаются ли страховые взносы суммы прощённого долга, необходимо учитывать нормы Налогового кодекса РФ и разъяснения контролирующих органов, а также внимательно изучить документы, на основании которых была осуществлена прощение долга.\n",
      "\n",
      "Для получения более точного ответа на ваш вопрос рекомендую обратиться к специалисту по бухгалтерскому учёту и налогообложению или в соответствующий государственный орган.\n",
      "--------------------------------------------------\n",
      "Пользователь: Расскажи мне про Курчатова\n",
      "\n",
      " Ассистент: [SEP] **Курчатов Игорь Васильевич** (1902–1960) — советский физик, организатор и руководитель работ по атомной науке и технике в СССР. Основатель и первый директор Института атомной энергии. Академик АН СССР (1943).\n",
      "\n",
      "Игорь Курчатов родился в 1902 году в посёлке Симский завод (ныне г. Сим, Челябинская область). В 1923 году окончил физико-математическое отделение физико-математического факультета Ростовского университета. Затем учился в Петроградском политехническом институте (ныне Санкт-Петербургский политехнический университет Петра Великого).\n",
      "\n",
      "Курчатов известен своими работами в области ядерной физики и создания ядерного оружия в СССР. Под его руководством были разработаны и созданы первые советские атомная и водородная бомбы. Также Курчатов внёс значительный вклад в развитие мирного использования атомной энергии, в частности, в создание первой в Евразии атомной электростанции в Обнинске.\n",
      "\n",
      "За свои достижения Курчатов был награждён множеством наград и званий, включая звание Героя Социалистического Труда и Сталинскую премию.\n",
      "\n",
      "Имя Курчатова носит множество научных и учебных заведений, в том числе Национальный исследовательский ядерный университет МИФИ в Москве.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c53495-8e6c-4a4c-9484-df5348e92df4",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c96d7ef-6b18-4fd7-bb06-00809638f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs type: <class 'int'>\n",
      "Attention mask type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs type:\", type(tokenized_dataset[0][\"input_ids\"][0]))\n",
    "print(\"Attention mask type:\", type(tokenized_dataset[0][\"attention_mask\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ca6eeff-24ad-4405-b985-32055226c0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Кастомный data collator для правильной обработки типов данных\n",
    "# class CustomDataCollator(DataCollatorForLanguageModeling):\n",
    "#     def __call__(self, features):\n",
    "#         batch = super().__call__(features)\n",
    "        \n",
    "#         # Преобразуем attention_mask в bool\n",
    "#         if 'attention_mask' in batch:\n",
    "#             batch['attention_mask'] = batch['attention_mask'].bool()\n",
    "        \n",
    "#         return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6093fd76-288c-4930-9fe5-ece8b6a1fd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "83f5c4ac-4d38-410a-aa1a-f20bf0c473ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./yandexgpt-lora-finetuned\",\n",
    "    per_device_train_batch_size=1, #2,\n",
    "    per_device_eval_batch_size=1, #2,\n",
    "    gradient_accumulation_steps=8, #4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1, #3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",  # Оценка после каждой эпохи\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",       # Важно для QLoRA\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3212ed16-84c2-4175-96c3-7f52c476a465",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False), # CustomDataCollator(tokenizer, mlm=False), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1695151e-b299-45fb-b176-09f54be3c069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Принудительно используем eager attention вместо SDPA\n",
    "model.config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d05ce3c-c4b7-4d39-99eb-2c3165337b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.86GB, reserved=4.18GB, peak=3.79GB\n",
      "NVML used=5.29GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ddf347b3-4480-454f-8e59-4ab5904ab868",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f684c50-f242-48c4-809c-a55dee6e169c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.86GB, reserved=4.15GB, peak=3.79GB\n",
      "NVML used=5.27GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9115d1f-00db-4f8a-8102-63767d287e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1115' max='1115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1115/1115 3:43:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.510600</td>\n",
       "      <td>1.981060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1115, training_loss=1.7642043417344713, metrics={'train_runtime': 13406.1206, 'train_samples_per_second': 0.665, 'train_steps_per_second': 0.083, 'total_flos': 4.135426241593344e+17, 'train_loss': 1.7642043417344713, 'epoch': 1.0})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "474c0231-09d7-41dc-b738-f9646bae6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.87GB, reserved=4.32GB, peak=3.79GB\n",
      "NVML used=5.54GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d6611ae3-5cfc-4ed4-88e1-7e340de90c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./yandexgpt-lora-finetuned/tokenizer_config.json',\n",
       " './yandexgpt-lora-finetuned/special_tokens_map.json',\n",
       " './yandexgpt-lora-finetuned/chat_template.jinja',\n",
       " './yandexgpt-lora-finetuned/tokenizer.model',\n",
       " './yandexgpt-lora-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./yandexgpt-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bab3f0-0b3f-4839-8637-f3e68340df04",
   "metadata": {},
   "source": [
    "### Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "950a5b14-c2c9-40c2-8120-6d157d1be946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, PeftConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f2024024-cfdd-4119-94b6-500609dee322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация для 4-битной загрузки (такая же как при обучении)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "50d58e25-453c-432d-9f73-f4fdf8c91f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Загрузка базовой модели с квантизацией\n",
    "model_name = \"yandex/YandexGPT-5-Lite-8B-instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "656153aa-c59a-41a7-ba7e-5e0dd8076078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ccf35b92-eb7f-40ed-9d90-3c1154229f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавляем паддинг токен если нужно\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a63ac2f-3ecd-4e5d-8ca7-29e967e8f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка LoRA адаптера\n",
    "lora_adapter_path = \"./yandexgpt-lora-finetuned\"  # путь к вашему адаптеру\n",
    "model = PeftModel.from_pretrained(base_model, lora_adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3d30aab0-62cb-40ae-8bb0-f9fc544589ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(129024, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=129024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d13e2117-3b59-4c72-9b3a-adf2e131f8b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пользователь: Как вкусно приготовить индейку на гриле?\n",
      "\n",
      " Ассистент: [SEP] Ассистент: [SEP] Ассистент: [SEP] Ассистент: Ты не мог бы поделиться рецептом вкусного маринада для индейки? Я хочу приготовить её на гриле, но не знаю, какие специи и приправы использовать.\n",
      "--------------------------------------------------\n",
      "Пользователь: Как распознать приближающийся инсульт?\n",
      "\n",
      " Ассистент: [SEP] Ассистент: [SEP] Ассистент: [SEP] Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Асси\n",
      "--------------------------------------------------\n",
      "Пользователь: Сформулируй основные каноны архитектуры древних цивилизаций\n",
      "\n",
      " Ассистент: [SEP] Ассистент: [SEP] Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент: Ассистент\n",
      "--------------------------------------------------\n",
      "Пользователь: Облагать ли страховыми взносами суммы прощенного долга по займу от организации где работает застрахованный?\n",
      "\n",
      " Ассистент: [SEP] Ассистент: [SEP] Ассистент: Ассистент: Сумма прощенного долга по займу от организации, где работает застрахованный, не облагается страховыми взносами, если она не является доходом застрахованного лица.\n",
      "\n",
      "Если сумма прощенного долга является доходом, то она облагается страховыми взносами в общем порядке.\n",
      "--------------------------------------------------\n",
      "Пользователь: Расскажи мне про Курчатова\n",
      "\n",
      " Ассистент: [SEP] Ассистент: [SEP] Ассистент: Ассистент: Игорь Васильевич Курчатов был выдающимся советским физиком, академиком АН СССР, одним из основателей и руководителей атомной науки в СССР. Он родился 12 января 1902 года и умер 7 февраля 1960 года.\n",
      "\n",
      "Курчатов внес значительный вклад в развитие ядерной физики и создание ядерного оружия в СССР. Он руководил созданием первой советской атомной бомбы и атомной энергетики. Под его руководством были проведены важные исследования в области физики атомного ядра, а также разработаны методы разделения изотопов.\n",
      "\n",
      "Курчатов также занимался теоретическими исследованиями и экспериментами, связанными с ядерной физикой, и был одним из первых, кто изучал свойства ядерного деления. Его работы оказали огромное влияние на развитие атомной науки и технологии в СССР и во всем мире.\n",
      "\n",
      "В честь Курчатова назван город Курчатов в Казахстане и несколько научных учреждений и объектов, связанных с атомной наукой и энергетикой.\n",
      "\n",
      " Ассистент: Ассистент: Игорь Васильевич Курчатов (1902–1960) — выдающийся советский физик, академик АН СССР, один из основателей атомной науки в СССР. Родился 12 января 1902 года, умер 7 февраля 1960 года.\n",
      "\n",
      "Внёс значительный вклад в развитие ядерной физики, руководил созданием первой советской атомной бомбы и атомной энергетики. Под его руководством были проведены важные исследования в области физики атомного ядра и разработаны методы разделения изотопов\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f72ccdea-ce67-45a0-8ddb-164f59139d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=6.38GB, reserved=8.95GB, peak=7.31GB\n",
      "NVML used=10.26GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf1d64-0916-4c50-ae82-51548a76b480",
   "metadata": {},
   "source": [
    "Буду благодарен если подскажете где я накосячил с шаблоном"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfda2450-466a-4988-a5bd-2448ef5aa85e",
   "metadata": {},
   "source": [
    ":("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60844011-2648-4e0b-8782-a7bf537b26ad",
   "metadata": {},
   "source": [
    "## Работа над ошибками - второй заход"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fe8dca53-4ac6-41e8-b02b-d8abdfbaaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    conversation = example[\"conversation\"]\n",
    "    \n",
    "    # Применяем чат-шаблон с токенизацией\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,  # Токенизируем сразу!\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Для causal LM метки такие же как input_ids\n",
    "    return {\n",
    "        \"input_ids\": tokenized,\n",
    "        \"attention_mask\": [1] * len(tokenized),  # Все токены значимые\n",
    "        \"labels\": tokenized.copy()  # Копируем для labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2ee404-24b0-4755-bea1-7d681a7f43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_prep = dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cfa906c2-a766-4dbf-85de-08bcd80b2d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 821.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "check_data = check_data_prep.map(formatting_func,\n",
    "                                 batched=False,\n",
    "                                 # batch_size=1000,\n",
    "                                 remove_columns=check_data_prep.column_names  # Удаляем оригинальные колонки\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c21d06f-75d5-437a-b254-d79785c0694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [1, 1, 16861, 125851, 1759, 1403, 52612, 26900, 2019, 5386]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length: 1024\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", tokenized_dataset[0][\"input_ids\"][:10])\n",
    "print(\"Attention mask:\", tokenized_dataset[0][\"attention_mask\"][:10])\n",
    "print(\"Length:\", len(tokenized_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3f9b9728-e7db-4946-b374-e2546b049bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "    conversation = example[\"conversation\"]\n",
    "    \n",
    "    # Применяем чат-шаблон БЕЗ обрезки\n",
    "    tokenized = tokenizer.apply_chat_template(\n",
    "        conversation,\n",
    "        tokenize=True,\n",
    "        truncation=False,  # Отключаем обрезку!\n",
    "        max_length=None,   # Без ограничения длины\n",
    "        padding=False,     # Не добавляем паддинг здесь\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    # Теперь добавляем паддинг отдельно\n",
    "    if len(tokenized) < 1024:\n",
    "        # Добавляем паддинг\n",
    "        padded = tokenized + [tokenizer.pad_token_id] * (1024 - len(tokenized))\n",
    "        attention_mask = [1] * len(tokenized) + [0] * (1024 - len(tokenized))\n",
    "    else:\n",
    "        # Обрезаем до максимальной длины\n",
    "        padded = tokenized[:1024]\n",
    "        attention_mask = [1] * 1024\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": padded,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": padded.copy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d78f6d62-3879-4403-8e1b-3bac6bbf281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func_final(example):\n",
    "    conversation = example[\"conversation\"]\n",
    "    \n",
    "    # Создаем полный диалог\n",
    "    full_dialog = \"<s>\"\n",
    "    for message in conversation:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            full_dialog += f\" Пользователь: {message['content']}\"\n",
    "        elif message[\"role\"] == \"assistant\":\n",
    "            full_dialog += f\" Ассистент: {message['content']}[SEP]\"\n",
    "    \n",
    "    # Токенизируем\n",
    "    tokenized = tokenizer(\n",
    "        full_dialog,\n",
    "        truncation=True,\n",
    "        max_length=2048,  # Достаточно для полных ответов\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"input_ids\"].copy()  # Для simple causal LM\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1abc8a27-fd3f-4909-8758-0610a17f5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_training_data():\n",
    "    print(\"Проверка подготовки данных для обучения:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i in range(min(3, len(dataset))):\n",
    "        sample = dataset[i]\n",
    "        tokenized = formatting_func_final(sample)\n",
    "        decoded = tokenizer.decode(tokenized[\"input_ids\"])\n",
    "        \n",
    "        print(f\"\\nПример {i+1}:\")\n",
    "        print(f\"Длина: {len([x for x in tokenized['input_ids'] if x != tokenizer.pad_token_id])} токенов\")\n",
    "        \n",
    "        # Проверяем ключевые элементы\n",
    "        has_user = \"Пользователь:\" in decoded\n",
    "        has_assistant = \"Ассистент:\" in decoded\n",
    "        has_sep = \"[SEP]\" in decoded\n",
    "        has_content = any(word in decoded for word in [\"страте\", \"игр\", \"ответ\"])\n",
    "        \n",
    "        print(f\"✓ Пользователь: {has_user}\")\n",
    "        print(f\"✓ Ассистент: {has_assistant}\")\n",
    "        print(f\"✓ [SEP]: {has_sep}\")\n",
    "        print(f\"✓ Контент: {has_content}\")\n",
    "        print(decoded)\n",
    "        \n",
    "        if all([has_user, has_assistant, has_sep, has_content]):\n",
    "            print(\"✓ Данные корректны для обучения\")\n",
    "        else:\n",
    "            print(\"✗ Проблема с данными\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a41e259-4d6b-4dc9-993f-b9e378156635",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка подготовки данных для обучения:\n",
      "============================================================\n",
      "\n",
      "Пример 1:\n",
      "Длина: 569 токенов\n",
      "✓ Пользователь: True\n",
      "✓ Ассистент: True\n",
      "✓ [SEP]: True\n",
      "✓ Контент: True\n",
      "<s><s> Пользователь: мне очень интересны стратегические игры, и я недавно узнал про игру ним. не мог бы ты объяснить мне стратегию оптимальной игры в ним? и еще, если есть, поделись интересным вариантом игры в крестики-нолики или другие стратегические головоломки, в которые мы могли бы сыграть вместе. как насчет того, чтобы рассмотреть 15 puzzle? мне бы хотелось узнать, есть ли для неё какая-то выигрышная стратегия или подход, который гарантирует победу. Ассистент: Расскажу тебе о стратегиях игры в Ним и затрону тему 15 Puzzle.\n",
      "\n",
      "### Стратегия оптимальной игры в Ним\n",
      "\n",
      "Игра Ним — это математическая игра, для которой существует чёткая выигрышная стратегия. Основа стратегии лежит в понятии ним-суммы — это побитовое исключающее ИЛИ (XOR) размеров кучек.\n",
      "\n",
      "Оптимальная стратегия заключается в следующем:\n",
      "\n",
      "1. Вычисли ним-сумму всех кучек.\n",
      "2. Если ним-сумма равна нулю, то ваше положение проигрышное при оптимальной игре соперника. В этом случае просто делайте ход и старайтесь создать трудности для соперника.\n",
      "3. Если ним-сумма не равна нулю, вам нужно сделать ход, который уменьшит размер одной из кучек так, чтобы новая ним-сумма всех кучек была равна нулю.\n",
      "4. После вашего хода соперник окажется в положении с ним-суммой равной нулю, что является проигрышным положением при вашей оптимальной игре.\n",
      "\n",
      "### Играем в крестики-нолики\n",
      "\n",
      "Что касается крестиков-ноликов, можно попробовать игру на расширенном поле 5x5 или 10x10, где для победы нужно выстроить ряд из 5 символов. Это добавит сложности и стратегической глубины.\n",
      "\n",
      "### 15 Puzzle (Пятнашки)\n",
      "\n",
      "15 Puzzle — это головоломка с плитками, которые нужно упорядочить по порядку, перемещая их в пустое пространство. \n",
      "\n",
      "Для 15 Puzzle существуют алгоритмы, которые позволяют определить решаемость пазла (не все начальные позиции решаемы) и выигрышная стратегия, если пазл решаем. Основной подход к решению:\n",
      "\n",
      "1. Разделите пазл на зоны и решайте поэтапно (например, сначала верхние две строки, затем левая половина и т.д.).\n",
      "2. Используйте алгоритмы для перестановки плиток, которые минимизируют количество нежелательных изменений (например, метод \"переворота угла\").\n",
      "3. Для каждого этапа запоминайте последовательность ходов, которая позволяет решать типичные задачи (например, как поменять местами две конкретные плитки).\n",
      "\n",
      "Сложность заключается в том, что без знания алгоритмов и последовательностей ходов решить пазл может быть довольно непросто, особенно если вы не имеете опыта с подобными головоломками. Но при наличии практики и понимания методов, пазл становится решаемым практически всегда. [SEP] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "✓ Данные корректны для обучения\n",
      "\n",
      "Пример 2:\n",
      "Длина: 563 токенов\n",
      "✓ Пользователь: True\n",
      "✓ Ассистент: True\n",
      "✓ [SEP]: True\n",
      "✓ Контент: True\n",
      "<s><s> Пользователь: слушай, у меня тут возникла задачка по архитектуре компьютера, и я не могу в ней разобраться. мне нужно разработать алгоритм, который оптимизирует доступ к кэш-памяти в многоядерном процессоре для параллельных вычислений. ты не мог бы помочь с этим? вот как я представляю задачу:\n",
      "\n",
      "1. имеется многоядерный процессор с общим кэшем второго уровня.\n",
      "2. необходимо минимизировать промахи кэша при параллельном выполнении нескольких интенсивных задач по обработке данных.\n",
      "3. алгоритм должен распределять данные таким образом, чтобы максимально использовать преимущества локальности данных и минимизировать конфликтные промахи.\n",
      "\n",
      "может есть идеи, как это можно организовать на уровне алгоритма? буду рад любым предложениям и советам! Ассистент: Ваша задача действительно интересная и актуальная в контексте современных многоядерных процессоров. Для оптимизации доступа к кэш-памяти важно учитывать такие аспекты, как пространственная и временная локальность данных, а также избегать конфликтов при кэшировании. Вот основные шаги, которые помогут в разработке алгоритма:\n",
      "\n",
      "### Шаг 1: Анализ задач\n",
      "Прежде всего, нужно проанализировать характеристики задач, которые будут выполняться параллельно. Важно понять, какие данные используются и как часто они запрашиваются.\n",
      "\n",
      "### Шаг 2: Разработка стратегии распределения данных\n",
      "Далее, необходимо разработать стратегию, которая позволит распределить данные таким образом, чтобы максимально использовать преимущества пространственной и временной локальности. Это может включать в себя:\n",
      "\n",
      "- **Группировка связанных данных**: чтобы при обращении к одному элементу данных рядом находились и другие часто используемые элементы.\n",
      "- **Разделение данных между ядрами**: чтобы каждое ядро работало с уникальным набором данных и минимизировать конфликтные промахи.\n",
      "\n",
      "### Шаг 3: Разбиение данных на блоки\n",
      "После того как стратегия распределения данных разработана, следует разбить данные на блоки оптимального размера для кэширования. В идеале, размер блока должен соответствовать размеру линейки кэша.\n",
      "\n",
      "### Шаг 4: Реализация алгоритма предвыборки (prefetching)\n",
      "Разработайте алгоритм предвыборки данных, который будет загружать данные в кэш заранее, основываясь на предсказании обращений к данным.\n",
      "\n",
      "### Шаг 5: Синхронизация доступа к данным\n",
      "Необходимо также обеспечить эффективную синхронизацию доступа к данным между ядрами, чтобы избежать ситуаций, когда несколько ядер одновременно обновляют данные в кэше.\n",
      "\n",
      "### Шаг 6: Тестирование и отладка\n",
      "Проведите тестирование алгоритма на различных наборах данных и рабочих нагрузках. Оцените эффективность алгоритма, подсчитывая количество промахов кэша и сравнивая с базовым случаем.\n",
      "\n",
      "### Шаг 7: Итерация и оптимизация\n",
      "На основе полученных результатов внесите коррективы в алгоритм и повторите тестирование. Ищите возможности для дальнейшей оптимизации.\n",
      "\n",
      "Эти шаги должны помочь вам в разработке эффективного алгоритма оптимизации доступа к кэш-памяти для параллельных вычислений. Успехов в работе! [SEP] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "✓ Данные корректны для обучения\n",
      "\n",
      "Пример 3:\n",
      "Длина: 940 токенов\n",
      "✓ Пользователь: True\n",
      "✓ Ассистент: True\n",
      "✓ [SEP]: True\n",
      "✓ Контент: True\n",
      "<s><s> Пользователь: Здравствуйте, помогите мне, пожалуйста, разобраться с одной тригонометрической функцией. Мне нужно найти основные значения арксинуса и арккосинуса, используя единичную окружность. Могли бы вы показать мне их на графике, и объяснить, как их можно использовать для решения уравнений? Кроме того, как применить эти функции для решения практических задач, например, в физике или инженерии? Буду очень признателен за подробный разбор с примерами. Ассистент: Здравствуйте! Давайте разберёмся с основными значениями арксинуса и арккосинуса на примере единичной окружности, а также рассмотрим их использование в уравнениях и практических задачах.\n",
      "\n",
      "### Основные значения арксинуса и арккосинуса на единичной окружности\n",
      "\n",
      "Единичная окружность — это окружность с радиусом равным 1, центр которой расположен в начале координат на координатной плоскости.\n",
      "\n",
      "#### Арксинус\n",
      "Функция арксинус (`arcsin`) — это обратная функция к синусу. Она позволяет определить угол по его синусу. Для арксинуса:\n",
      "- Определение: `arcsin(y) = x` ↔ `sin(x) = y`\n",
      "- Область значений функции арксинуса: `[-π/2, π/2]` или `[-90°, 90°]`\n",
      "\n",
      "#### Арккосинус\n",
      "Функция арккосинус (`arccos`) — это также обратная функция, но к косинусу. Она позволяет определить угол по его косинусу. Для арккосинуса:\n",
      "- Определение: `arccos(y) = x` ↔ `cos(x) = y`\n",
      "- Область значений функции арккосинуса: `[0, π]` или `[0°, 180°]`\n",
      "\n",
      "На графике единичной окружности значения арксинуса и арккосинуса можно представить как углы относительно оси X. К сожалению, я не могу предоставить вам изображение, но вы можете визуализировать это, нарисовав окружность и отметив на ней углы, соответствующие значениям из областей значений функций.\n",
      "\n",
      "### Использование арксинуса и арккосинуса в уравнениях\n",
      "\n",
      "Для решения уравнений вида `sin(x) = y` и `cos(x) = y` используются соответственно арксинус и арккосинус. Например:\n",
      "\n",
      "1. `sin(x) = 1/2`\n",
      "   Чтобы найти `x`, используем арксинус: `x = arcsin(1/2)`. Зная область значений арксинуса, получаем `x = π/6` или `x = 30°`.\n",
      "\n",
      "2. `cos(x) = -1`\n",
      "   Используем арккосинус: `x = arccos(-1)`. Зная область значений арккосинуса, получаем `x = π` или `x = 180°`.\n",
      "\n",
      "### Применение в практических задачах\n",
      "\n",
      "Арксинус и арккосинус могут использоваться в различных областях, включая физику и инженерию, для определения углов по известным отношениям сторон в треугольниках.\n",
      "\n",
      "#### Пример в физике:\n",
      "Рассмотрим задачу нахождения угла броска при движении тела по параболической траектории при известной начальной скорости `v` и дистанции `d` до точки падения.\n",
      "\n",
      "1. Уравнение для максимального расстояния, которое тело пролетит при угле броска `θ`, выглядит так: `d = (v^2 * sin(2θ)) / g`, где `g` — ускорение свободного падения.\n",
      "2. Выразим `sin(2θ)`: `sin(2θ) = d * g / v^2`.\n",
      "3. Для нахождения угла `θ` используем арксинус: `2θ = arcsin(d * g / v^2)`.\n",
      "4. Отсюда `θ = 1/2 * arcsin(d * g / v^2)`.\n",
      "\n",
      "#### Пример в инженерии:\n",
      "Пусть требуется определить угол наклона лестницы, опирающейся на стену, при известной высоте `h`, на которую она опирается, и длине лестницы `l`.\n",
      "\n",
      "1. Используя тригонометрические соотношения в прямоугольном треугольнике, определяем, что `cos(θ) = h / l`.\n",
      "2. Чтобы найти угол `θ`, используем арккосинус: `θ = arccos(h / l)`.\n",
      "\n",
      "Надеюсь, эти примеры помогли вам понять, как использовать арксинус и арккосинус для решения практических задач. Если у вас возникнут дополнительные вопросы, не стесняйтесь их задать! [SEP] </s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "✓ Данные корректны для обучения\n"
     ]
    }
   ],
   "source": [
    "verify_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff75ea-774d-4be8-aa63-3f6af330305c",
   "metadata": {},
   "source": [
    "### Подготовка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cc59e55d-5244-4334-8099-df62f702708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA before train] allocated=2.85GB, reserved=4.15GB, peak=3.79GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA before train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "473a29fe-469b-4ab4-8a90-c4b94819eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data_prep = dataset.select(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "106e0802-751a-4c70-aeff-63237ff9cda0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5/5 [00:00<00:00, 275.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "check_data = check_data_prep.map(formatting_func_final,\n",
    "                                 batched=False,\n",
    "                                 # batch_size=1000,\n",
    "                                 remove_columns=check_data_prep.column_names  # Удаляем оригинальные колонки\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b682a54-6d38-493b-8310-c972665aeb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 5\n",
       "})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "002a54cf-408b-4aa8-a693-d8cc14ccb033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9906/9906 [00:13<00:00, 710.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    formatting_func_final,\n",
    "    batched=False,\n",
    "    remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6df00f3-619e-4b4d-ade3-647d2d45480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12c71c9c-f233-4ab1-a928-418312c29b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: [1, 1, 16861, 125851, 1759, 1403, 52612, 26900, 2019, 5386]\n",
      "Attention mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "Length: 2048\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs:\", tokenized_dataset[0][\"input_ids\"][:10])\n",
    "print(\"Attention mask:\", tokenized_dataset[0][\"attention_mask\"][:10])\n",
    "print(\"Length:\", len(tokenized_dataset[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "21ec0976-ada6-4b61-bae4-490da14beefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_for_test = [\n",
    "    'Как вкусно приготовить индейку на гриле?',\n",
    "    'Как распознать приближающийся инсульт?',\n",
    "    'Сформулируй основные каноны архитектуры древних цивилизаций',\n",
    "    'Облагать ли страховыми взносами суммы прощенного долга по займу от организации где работает застрахованный?',\n",
    "    'Расскажи мне про Курчатова'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6d426f8a-1de3-4e5b-95fe-98d9961b2855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_correct(prompt):\n",
    "    # Форматируем только пользовательский промпт\n",
    "    user_prompt = f\"<s> Пользователь: {prompt} Ассистент:\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        user_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Извлекаем только сгенерированную часть\n",
    "    generated = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Убираем возможный [SEP] в конце\n",
    "    response = response.replace(\"[SEP]\", \"\").strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d474931-462d-420c-82e8-0c274c789541",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Индейка на гриле: пошаговый рецепт**\n",
      "\n",
      "**Ингредиенты:**\n",
      "* Индейка (филе или другие части) — 1 кг;\n",
      "* Соль — по вкусу;\n",
      "* Чёрный перец (молотый) — по вкусу;\n",
      "* Чеснок — 3–4 зубчика;\n",
      "* Розмарин свежий — 1 веточка;\n",
      "* Оливковое масло — 2–3 ст. л.;\n",
      "* Лимонный сок — 2–3 ст. л.;\n",
      "* Соевый соус — 1–2 ст. л. (по желанию);\n",
      "* Специи для птицы (по желанию) — по вкусу.\n",
      "\n",
      "**Приготовление:**\n",
      "\n",
      "1. Подготовьте индейку: промойте и обсушите бумажным полотенцем. Нарежьте индейку на порционные куски или оставьте целиком, в зависимости от ваших предпочтений.\n",
      "\n",
      "2. Чеснок очистите и пропустите через пресс.\n",
      "\n",
      "3. В небольшой миске смешайте оливковое масло, лимонный сок, соевый соус, измельчённый чеснок, соль, перец и розмарин.\n",
      "\n",
      "4. Полученным маринадом тщательно обмажьте индейку со всех сторон.\n",
      "\n",
      "5. Оставьте мариноваться минимум на 30 минут (можно и на более длительный срок).\n",
      "\n",
      "6. Разогрейте гриль. Если вы используете угольный гриль, дайте углям прогореть до серого цвета.\n",
      "\n",
      "7. Выложите индейку на гриль. Жарьте на среднем\n",
      "--------------------------------------------------\n",
      "*При появлении любых симптомов, указывающих на инсульт, необходимо незамедлительно обратиться за медицинской помощью. Только врач может провести диагностику и назначить необходимое лечение.*\n",
      "\n",
      "**Симптомы инсульта могут включать:**\n",
      "\n",
      "- Внезапную слабость или онемение в лице, руке или ноге, особенно с одной стороны тела;\n",
      "- Внезапные проблемы со зрением, такие как потеря зрения в одном или обоих глазах, двоение в глазах;\n",
      "- Трудности с речью, например, невнятная речь или невозможность подобрать нужные слова;\n",
      "- Внезапное головокружение, нарушение координации или потеря равновесия;\n",
      "- Внезапную сильную головную боль, часто с чувством распирания или давления в голове;\n",
      "- Онемение конечностей;\n",
      "- Спутанность сознания, дезориентация.\n",
      "\n",
      "Важно помнить, что некоторые из этих симптомов могут появляться и исчезать, поэтому даже кратковременные проявления могут быть сигналом серьёзного заболевания. Если вы заметили у себя или у кого-то из близких подобные симптомы, срочно вызовите скорую медицинскую помощь.\n",
      "--------------------------------------------------\n",
      "1. **Египет**:\n",
      "- строгая симметрия и геометричность форм;\n",
      "- использование крупных блоков камня для строительства;\n",
      "- применение колонн и пилонов для поддержки конструкций;\n",
      "- монументальность и масштабность зданий;\n",
      "- иероглифические надписи и изображения на стенах храмов и гробниц;\n",
      "- поклонение природе и божествам через архитектурные формы и символы.\n",
      "\n",
      "2. **Древняя Греция**:\n",
      "- гармоничные пропорции и баланс;\n",
      "- использование мрамора и других натуральных материалов;\n",
      "- ордерная система (дорический, ионический, коринфский ордера);\n",
      "- храмы и театры с открытыми пространствами для общественных собраний;\n",
      "- акрополь как центр города;\n",
      "- скульптуры и рельефы на фасадах зданий.\n",
      "\n",
      "3. **Древний Рим**:\n",
      "- арочные и сводчатые конструкции;\n",
      "- использование бетона для строительства;\n",
      "- акведуки, мосты и дороги для обеспечения инфраструктуры;\n",
      "- амфитеатры и термы для общественных мероприятий;\n",
      "- арки и колонны для поддержки тяжёлых конструкций;\n",
      "- монументальность и величие в архитектуре.\n",
      "\n",
      "4. **Древний Китай**:\n",
      "- симметрия и регулярность планировки;\n",
      "- использование дерева и кирпича для строительства;\n",
      "- пагода как символ буддийской архитектуры;\n",
      "- соединение природных элементов с архитектурными формами;\n",
      "- важность гармонии с природой и космосом.\n",
      "\n",
      "5. **Древняя Месопотами\n",
      "--------------------------------------------------\n",
      "Согласно статье 415 Гражданского кодекса Российской Федерации, прощение долга является одним из способов прекращения обязательств.\n",
      "\n",
      "При этом, в соответствии с пунктом 1 статьи 423 Гражданского кодекса РФ, договор займа, по которому одна из сторон обязуется предоставить другой стороне деньги или вещи, определённые родовыми признаками, является договором возмездным, если из закона, иных правовых актов, содержания или существа договора не вытекает иное.\n",
      "\n",
      "Что касается страховых взносов, то в соответствии с пунктом 1 статьи 4221 Налогового кодекса РФ, объектом обложения страховыми взносами признаются выплаты и иные вознаграждения в пользу физических лиц, производимые в рамках трудовых отношений и гражданско-правовых договоров, предметом которых является выполнение работ, оказание услуг.\n",
      "\n",
      "Прощение долга по займу, как правило, не является выплатой или вознаграждением в рамках трудовых отношений. Поэтому, если прощение долга не связано с выполнением работником трудовых обязанностей и не предусмотрено трудовым договором или внутренними локальными актами организации, то суммы прощённого долга не облагаются страховыми взносами.\n",
      "\n",
      "Однако, если прощение долга имеет признаки выплаты, связанной с трудовыми отношениями (например, если это часть корпоративного договора, регулирующего отношения между работодателем и работником), то такие суммы могут быть признаны объектом обложения страховыми взносами.\n",
      "\n",
      "Для точного ответа на вопрос о необходимости обложения сумм прощённого долга страховыми взносами необходимо учитывать конкретные обстоятельства и условия сделки, а также правовую природу отношений между сторонами. Рекомендуется обратиться к специалисту в области налогового\n",
      "--------------------------------------------------\n",
      "Курчатов Игорь Васильевич (1902–1960) — советский физик, академик АН СССР (1943), организатор и руководитель работ по атомной науке и технике в СССР, основатель и первый директор Института атомной энергии (1943).\n",
      "\n",
      "Курчатов родился в посёлке Сим (ныне — город Сим, Челябинская область) в семье помощника лесничего. После окончания школы в 1919 году работал сначала учителем, затем чертёжником, расчётчиком, инженером в различных организациях.\n",
      "\n",
      "В 1923 году Курчатов поступил в Петроградский политехнический институт, где начал заниматься физикой под руководством академика А. Ф. Иоффе. В 1930 году он был приглашён на работу в Ленинградский физико-технический институт, где организовал лабораторию ядерных исследований и вёл экспериментальную работу по физике твёрдого тела.\n",
      "\n",
      "В 1932 году Курчатов начал исследования по расщеплению ядра. В 1935 году вместе с сотрудниками впервые в СССР расщепили ядро атома лития.\n",
      "\n",
      "С началом Великой Отечественной войны Курчатов начал работу по созданию атомной бомбы. В 1942 году он возглавил лабораторию, которая занималась разработкой ядерного оружия. В 1945 году под его руководством был запущен первый советский циклотрон — первый в Европе атомный реактор. В 1946 году был запущен первый в Европе атомный реактор Ф-1.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer_correct(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20343904-1c95-40b4-9608-916bb205d43d",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8697d42d-d783-42dc-b25e-3fd396b56b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs type: <class 'int'>\n",
      "Attention mask type: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input IDs type:\", type(tokenized_dataset[0][\"input_ids\"][0]))\n",
    "print(\"Attention mask type:\", type(tokenized_dataset[0][\"attention_mask\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c06dc27-afa2-4ede-8e20-46870b13ecca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset_split[\"train\"]\n",
    "eval_dataset = dataset_split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ea396750-153e-4bf2-b6a4-474bb4563d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./yandexgpt-lora-finetuned\",\n",
    "    per_device_train_batch_size=1, #2,\n",
    "    per_device_eval_batch_size=1, #2,\n",
    "    gradient_accumulation_steps=8, #4,\n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=1, #3,\n",
    "    \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,           # Логировать каждые 5 шагов\n",
    "    logging_first_step=True,   # Логировать первый шаг\n",
    "    logging_strategy=\"steps\",  # Логировать по шагам\n",
    "    \n",
    "    eval_strategy=\"steps\",  # Оценка по шагам вместо эпох\n",
    "    eval_steps=50,               # Оценивать каждые 50 шагов\n",
    "\n",
    "    log_level=\"info\",           # Более подробный уровень логирования\n",
    "    disable_tqdm=False,         # Включить прогресс-бар\n",
    "    \n",
    "    save_strategy=\"steps\",       # Сохранять по шагам\n",
    "    save_steps=100,              # Сохранять каждые 100 шагов\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",       # Важно для QLoRA\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3fe6a9ba-ab7a-4d64-8371-761db18c74eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False), # CustomDataCollator(tokenizer, mlm=False), \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b7f0661-1b84-4bd8-92b8-ad6c1d4b6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config._attn_implementation = \"eager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ea620b50-2774-4c84-a6b5-b0d0827bea22",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "485034b9-7c00-476b-9253-e472dc1f69d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.86GB, reserved=4.15GB, peak=3.79GB\n",
      "NVML used=5.33GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e78705e-2304-4dbe-aa2e-43a4f7bbd27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipped Embedding(129024, 4096): 504.0M params\n",
      "skipped: 504.0M params\n",
      "***** Running training *****\n",
      "  Num examples = 8,915\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 1,115\n",
      "  Number of trainable parameters = 41,943,040\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1115' max='1115' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1115/1115 14:09:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.129100</td>\n",
       "      <td>1.239659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.132100</td>\n",
       "      <td>1.199858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.196600</td>\n",
       "      <td>1.190117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.189400</td>\n",
       "      <td>1.185444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.102200</td>\n",
       "      <td>1.181259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.158700</td>\n",
       "      <td>1.177102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.161700</td>\n",
       "      <td>1.174204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.004000</td>\n",
       "      <td>1.171757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.180000</td>\n",
       "      <td>1.170132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.125000</td>\n",
       "      <td>1.166675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.156100</td>\n",
       "      <td>1.165008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.032400</td>\n",
       "      <td>1.162337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.087700</td>\n",
       "      <td>1.161175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.033900</td>\n",
       "      <td>1.158879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.050000</td>\n",
       "      <td>1.157367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.091300</td>\n",
       "      <td>1.156302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.972900</td>\n",
       "      <td>1.155033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.178800</td>\n",
       "      <td>1.153673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.088700</td>\n",
       "      <td>1.152290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>1.151331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.988400</td>\n",
       "      <td>1.150913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.121400</td>\n",
       "      <td>1.150435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-100\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-100/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-200\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-200/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-200/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-200/special_tokens_map.json\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-300\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-300/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-300/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-300/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-100] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-400\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-400/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-400/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-400/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-200] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-500\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-500/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-300] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-600\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-600/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-600/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-600/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-400] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-700\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-700/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-700/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-700/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-500] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-800\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-800/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-800/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-800/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-600] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-900\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-900/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-900/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-900/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-700] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-1000\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-1000/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-800] due to args.save_total_limit\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 991\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-1100\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-1100/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-1100/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-1100/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-900] due to args.save_total_limit\n",
      "Saving model checkpoint to ./yandexgpt-lora-finetuned/checkpoint-1115\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/checkpoint-1115/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/checkpoint-1115/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/checkpoint-1115/special_tokens_map.json\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-1000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./yandexgpt-lora-finetuned/checkpoint-1100 (score: 1.150435209274292).\n",
      "Deleting older checkpoint [yandexgpt-lora-finetuned/checkpoint-1115] due to args.save_total_limit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1115, training_loss=1.080232146930267, metrics={'train_runtime': 51017.2504, 'train_samples_per_second': 0.175, 'train_steps_per_second': 0.022, 'total_flos': 8.270852483186688e+17, 'train_loss': 1.080232146930267, 'epoch': 1.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93a3e3bc-4a99-4502-9a23-c6c5a625bbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.87GB, reserved=7.30GB, peak=5.41GB\n",
      "NVML used=8.58GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "89e91db9-cc43-4edc-9b05-0ee8fb95ca9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./yandexgpt-lora-finetuned\n",
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "Saving Trainer.data_collator.tokenizer by default as Trainer.processing_class is `None`\n",
      "chat template saved in ./yandexgpt-lora-finetuned/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/special_tokens_map.json\n",
      "chat template saved in ./yandexgpt-lora-finetuned/chat_template.jinja\n",
      "tokenizer config file saved in ./yandexgpt-lora-finetuned/tokenizer_config.json\n",
      "Special tokens file saved in ./yandexgpt-lora-finetuned/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./yandexgpt-lora-finetuned/tokenizer_config.json',\n",
       " './yandexgpt-lora-finetuned/special_tokens_map.json',\n",
       " './yandexgpt-lora-finetuned/chat_template.jinja',\n",
       " './yandexgpt-lora-finetuned/tokenizer.model',\n",
       " './yandexgpt-lora-finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(\"./yandexgpt-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d6c27-2e80-4afb-b73a-a37deaf073ef",
   "metadata": {},
   "source": [
    "### Оценка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "11c2bebc-aff7-46ae-aa94-0978997240ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=2.87GB, reserved=7.30GB, peak=5.41GB\n",
      "NVML used=8.59GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a63e4c00-1cca-4ebb-9030-a4d33aa68f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Конфигурация для 4-битной загрузки (такая же как при обучении)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5321a252-3732-4eb5-a69d-ef81881da2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/config.json\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dtype\": \"float16\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 129024\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/model.safetensors.index.json\n",
      "Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:07<00:00,  1.77s/it]\n",
      "All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "All the weights of LlamaForCausalLM were initialized from the model checkpoint at yandex/YandexGPT-5-Lite-8B-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"temperature\": 0.3,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"yandex/YandexGPT-5-Lite-8B-instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe369f62-ff0b-4ab4-9fa7-ede6a261bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file tokenizer.model from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/tokenizer.model\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/viv232/.cache/huggingface/hub/models--yandex--YandexGPT-5-Lite-8B-instruct/snapshots/b556811768376b46c69caab60c4d1b69df9faaa1/tokenizer_config.json\n",
      "loading file tokenizer.json from cache at None\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    }
   ],
   "source": [
    "tokenizer_lr = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    use_fast=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b952e542-0364-461c-a3da-76573d15c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer_lr.pad_token is None:\n",
    "    tokenizer_lr.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "90f7aeaa-9886-46d1-8184-1eec818fd3c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(129024, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=129024, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_adapter_path = \"./yandexgpt-lora-finetuned\"  # путь к вашему адаптеру\n",
    "model_lr = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
    "model_lr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "49b03022-3eee-45fe-b598-053b0324a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_correct_lr(prompt):\n",
    "    # Форматируем только пользовательский промпт\n",
    "    user_prompt = f\"<s> Пользователь: {prompt} Ассистент:\"\n",
    "    \n",
    "    inputs = tokenizer_lr(\n",
    "        user_prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=1024\n",
    "    ).to(model_lr.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model_lr.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer_lr.pad_token_id,\n",
    "            eos_token_id=tokenizer_lr.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Извлекаем только сгенерированную часть\n",
    "    generated = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    response = tokenizer_lr.decode(generated, skip_special_tokens=True)\n",
    "    \n",
    "    # Убираем возможный [SEP] в конце\n",
    "    response = response.replace(\"[SEP]\", \"\").strip()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6be28a66-517a-4810-a20a-88bf2a0be2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=8.36GB, reserved=12.34GB, peak=8.51GB\n",
      "NVML used=13.66GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d20be388-a445-47d5-a595-64e02d7da11f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Для приготовления вкусной индейки на гриле вам понадобится следовать нескольким шагам. Вот простой рецепт приготовления индейки на гриле:\n",
      "\n",
      "### Ингредиенты:\n",
      "- Индейка (желательно филе) – 1 кг\n",
      "- Соль – по вкусу\n",
      "- Черный перец – по вкусу\n",
      "- Приправа для птицы – по вкусу\n",
      "- Оливковое масло – для смазывания\n",
      "\n",
      "### Инструкция:\n",
      "\n",
      "#### Подготовка индейки:\n",
      "1. **Разморозьте индейку** (если она заморожена), если это необходимо.\n",
      "2. **Нарежьте индейку** на порционные кусочки.\n",
      "3. **Смешайте специи с маслом**: в глубокой миске смешайте соль, перец, приправу для птицы и немного оливкового масла.\n",
      "4. **Маринуйте индейку**: положите кусочки индейки в миску со смесью специй и хорошо перемешайте, чтобы каждый кусочек был покрыт маринадом. Оставьте мариноваться на 30 минут.\n",
      "\n",
      "#### Гриль:\n",
      "1. **Разогрейте гриль**: предварительно разогрейте гриль до средней температуры.\n",
      "2. **Смажьте решетку гриля**: смажьте решетку гриля оливковым маслом, чтобы индейка не прилипала.\n",
      "3. **Жарьте индейку**: аккуратно выложите маринованные кусочки индейки на гриль. Жарьте с каждой стороны примерно по 5-7 минут, переворачивая, чтобы получить равномерную золотистую корочку.\n",
      "4. **Проверьте готовность**: индейка готова, когда её внутренняя температура достигает 75 градусов Цельсия или\n",
      "--------------------------------------------------\n",
      "Инсульт – это острое нарушение мозгового кровообращения, которое может привести к серьезным последствиям. Важно уметь распознавать признаки приближающегося инсульта, чтобы вовремя обратиться за медицинской помощью. Вот основные признаки, на которые следует обратить внимание:\n",
      "\n",
      "1. **Слабость или онемение** в лице, руке или ноге, особенно на одной стороне тела.\n",
      "2. **Спутанность сознания** или проблемы с речью, включая внезапное затруднение в подборе слов или понимании речи.\n",
      "3. **Проблемы со зрением**, такие как внезапное ухудшение зрения в одном или обоих глазах, двоение в глазах.\n",
      "4. **Головная боль**, которая может быть сильной и внезапной, не похожая на предыдущие.\n",
      "5. **Трудности с координацией** и равновесием, внезапные проблемы с ходьбой.\n",
      "6. **Потеря сознания** или кратковременные потери сознания.\n",
      "\n",
      "Если вы заметили у себя или у кого-то из вашего окружения один или несколько из этих симптомов, необходимо срочно вызвать скорую помощь. Не игнорируйте эти признаки, так как промедление может привести к необратимым последствиям.\n",
      "\n",
      "Помните, что чем раньше начато лечение инсульта, тем больше шансов на полное восстановление.\n",
      "--------------------------------------------------\n",
      "Основные каноны архитектуры древних цивилизаций - это правила и принципы, которыми руководствовались архитекторы и строители в создании архитектурных сооружений. Эти каноны варьировались в зависимости от культурного и исторического контекста каждой цивилизации, но можно выделить несколько общих черт, которые характерны для многих древних культур.\n",
      "\n",
      "### Египет\n",
      "1. **Строгие пропорции**: Египетские храмы и пирамиды строились с соблюдением строгих математических пропорций, что придавало им величественный и монументальный вид.\n",
      "2. **Симметрия и регулярность**: Египетская архитектура отличалась симметрией и регулярностью форм, что делало её узнаваемой.\n",
      "3. **Использование камня**: Здания возводились из камня, что обеспечивало их долговечность.\n",
      "4. **Иероглифы и рельефы**: Стены храмов и гробниц украшались иероглифами и рельефами, рассказывающими о жизни фараонов и богов.\n",
      "\n",
      "### Греция\n",
      "1. **Гармония и симметрия**: Греческая архитектура воплощала принципы гармонии и симметрии, что отражало идеалы красоты и совершенства.\n",
      "2. **Дорический, ионический и коринфский ордера**: Греческие храмы и здания строились с использованием различных архитектурных ордеров, каждый из которых имел свои уникальные черты.\n",
      "3. **Статуи и скульптурные украшения**: Здания часто украшались статуями и рельефами, изображающими богов, героев и мифологические сцены.\n",
      "4. **Акрополь**: Важной частью греческой архитектуры был акрополь,\n",
      "--------------------------------------------------\n",
      "Вопрос о налогообложении сумм прощенного долга по займу от организации, в которой работает застрахованный, связан с особенностями налогообложения в Российской Федерации. Рассмотрим этот вопрос пошагово.\n",
      "\n",
      "1. **Прощение долга как доход:**\n",
      "   Прощение долга рассматривается как доход, полученный физическим лицом, который подлежит налогообложению. Это регулируется статьей 209 Налогового кодекса РФ.\n",
      "\n",
      "2. **НДФЛ:**\n",
      "   Прощенный долг подлежит обложению налогом на доходы физических лиц (НДФЛ). Стандартная ставка НДФЛ для физических лиц составляет 13%, если только не предусмотрено иное (например, для резидентов РФ).\n",
      "\n",
      "3. **Страховые взносы:**\n",
      "   Страховые взносы - это платежи, которые работодатель делает в пользу работника для обеспечения пенсионного, социального и медицинского страхования.\n",
      "\n",
      "4. **Связь прощенного долга и страховых взносов:**\n",
      "   Прощенный долг сам по себе не является основанием для начисления страховых взносов, так как страховые взносы начисляются на доходы, которые были получены в рамках трудовых отношений.\n",
      "\n",
      "5. **Исключения:**\n",
      "   Однако, если прощение долга приводит к изменению условий трудовых отношений (например, изменение зарплаты или должности), то изменения могут повлечь за собой необходимость перерасчета страховых взносов.\n",
      "\n",
      "6. **Заключение:**\n",
      "   Таким образом, прощение долга по займу от организации не облагается страховыми взносами, так как страховые взносы начисляются только на доходы, полученные в рамках трудовых отношений. Однако, прощенный долг подлежит налогообложению НДФЛ, если он был признан доходом\n",
      "--------------------------------------------------\n",
      "Игорь Васильевич Курчатов (1902–1960) был выдающимся советским физиком, который сыграл ключевую роль в создании атомной и водородной бомб, а также в развитии атомной энергетики в СССР.\n",
      "\n",
      "### Ранние годы и образование\n",
      "Игорь Курчатов родился 12 января 1902 года в Симбирске (нынешний Ульяновск), Россия. Он получил образование в физико-математическом факультете Крымского университета, а затем в Ленинградском политехническом институте.\n",
      "\n",
      "### Научная карьера\n",
      "После окончания учебы Курчатов начал свою научную карьеру в области физики. Он работал в различных научных институтах и занимался исследованиями в области ядерной физики.\n",
      "\n",
      "### Создание атомной бомбы\n",
      "В 1940-х годах Курчатов возглавил работу по созданию атомной бомбы в СССР. Под его руководством были разработаны и проведены необходимые эксперименты, что позволило СССР успешно провести первое испытание атомной бомбы в 1949 году.\n",
      "\n",
      "### Создание водородной бомбы\n",
      "В 1950-х годах Курчатов также руководил работами по созданию водородной бомбы. СССР успешно провел первое испытание водородной бомбы в 1953 году.\n",
      "\n",
      "### Развитие атомной энергетики\n",
      "Курчатов также внес значительный вклад в развитие атомной энергетики в СССР. Он активно участвовал в разработке и строительстве первых советских атомных электростанций.\n",
      "\n",
      "### Смерть и наследие\n",
      "Игорь Курчатов скончался 7 февраля 1960 года в Москве. Его вклад в ядерную физи\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for text in prompts_for_test:\n",
    "    print(generate_answer_correct_lr(text))\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee0e83-e509-46ed-a1c5-9ac4ebd80440",
   "metadata": {},
   "source": [
    "# Дообучение энкодера e5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a9b58575-aa3e-46f1-ab85-4c6668ea06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, random, math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sentence_transformers import SentenceTransformer, losses, models\n",
    "from sentence_transformers.training_args import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from bitsandbytes.optim import AdamW8bit\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f15ef1d-4cd8-4c56-9243-02ec909c32df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d193c3b7-52b9-4db3-a366-2b62572a3a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flush():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17bc5d-be16-4def-91c3-15a31f61e3c0",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2bdcb49b-4b52-4b78-b2a6-42a345b424db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 45328/45328 [00:00<00:00, 614055.00 examples/s]\n",
      "Generating validation split: 100%|██████████| 5036/5036 [00:00<00:00, 397121.87 examples/s]\n",
      "Generating test split: 100%|██████████| 23936/23936 [00:00<00:00, 1033879.41 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train pairs: 2000 | Val pairs: 500\n",
      "Sample train pair: ('где в основном российские метрополитены расположены', 'Кроме того, Максимом Горьким в Городе Жёлтого Дьявола было введено в русский язык слово-калька подземка . Оно прижилось, но преимущественно в качестве обозначения зарубежных метрополитенов (лондонская подземка, нью-йоркская подземка и т. д.), хотя в последнее время встречается в российской прессе и применительно к российским метрополитенам, проложенным в основном под землёй. Соответственно, преимущественно эстакадные метрополитены называют надземками , несмотря на то, что таких метрополитенов в России пока ещё нет.')\n"
     ]
    }
   ],
   "source": [
    "def extract_pairs(ds, max_samples=None, seed=42):\n",
    "    pairs = []\n",
    "    for ex in ds:\n",
    "        q = ex.get(\"question\")\n",
    "        pos = ex.get(\"context\")\n",
    "        pairs.append((q, pos))\n",
    "\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        random.shuffle(pairs)\n",
    "\n",
    "    if max_samples is not None:\n",
    "        pairs = pairs[:max_samples]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "ds_train = load_dataset(\"kuznetsoffandrey/sberquad\", split=\"train[:2000]\")\n",
    "ds_val = load_dataset(\"kuznetsoffandrey/sberquad\", split=\"validation[:500]\")\n",
    "\n",
    "train_pairs = extract_pairs(ds_train)\n",
    "val_pairs   = extract_pairs(ds_val)\n",
    "\n",
    "print(f\"Train pairs: {len(train_pairs)} | Val pairs: {len(val_pairs)}\")\n",
    "print(\"Sample train pair:\", train_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b996a3-db2d-43e3-9d7c-80df487d544b",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "12ba2a2d-0a8c-45a8-9eac-21dd5142a972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/model.safetensors\n",
      "All model checkpoint weights were used when initializing XLMRobertaModel.\n",
      "\n",
      "All the weights of XLMRobertaModel were initialized from the model checkpoint at intfloat/multilingual-e5-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use XLMRobertaModel for predictions without further training.\n",
      "loading file sentencepiece.bpe.model from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/sentencepiece.bpe.model\n",
      "loading file tokenizer.json from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7,110,656 || all params: 567,001,088 || trainable%: 1.2541\n"
     ]
    }
   ],
   "source": [
    "flush()\n",
    "\n",
    "base_name = \"intfloat/multilingual-e5-large\"\n",
    "st_model = SentenceTransformer(base_name, device=device)\n",
    "\n",
    "# Извлекаем базовый AutoModel\n",
    "backbone = st_model[0].auto_model\n",
    "\n",
    "# Включаем gradient checkpointing\n",
    "if hasattr(backbone, \"gradient_checkpointing_enable\"):\n",
    "    backbone.gradient_checkpointing_enable()\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
    "    task_type=TaskType.FEATURE_EXTRACTION,\n",
    ")\n",
    "peft_backbone = get_peft_model(backbone, lora_cfg)\n",
    "peft_backbone.print_trainable_parameters()\n",
    "\n",
    "st_model[0].auto_model = peft_backbone\n",
    "\n",
    "loss_fn = losses.MultipleNegativesRankingLoss(st_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4bd646d8-bae5-4714-a959-180680825f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@5: 0.984\n"
     ]
    }
   ],
   "source": [
    "def embed(texts, model, batch_size=128, normalize=True):\n",
    "    vectors = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=normalize,\n",
    "        device=device,\n",
    "        show_progress_bar=False,\n",
    "    )\n",
    "    return vectors\n",
    "\n",
    "queries = [q for q,_ in val_pairs]\n",
    "docs    = [d for _,d in val_pairs]\n",
    "\n",
    "q_vecs = embed(queries, st_model)\n",
    "d_vecs = embed(docs, st_model)\n",
    "\n",
    "sims = np.matmul(q_vecs, d_vecs.T)\n",
    "k = min(5, sims.shape[1])\n",
    "topk_idx = np.argpartition(-sims, kth=k-1, axis=1)[:, :k]\n",
    "\n",
    "true_idx = np.arange(len(val_pairs))\n",
    "hits = (topk_idx == true_idx[:, None]).any(axis=1)\n",
    "hit5 = hits.mean()\n",
    "print(f\"Hit@5: {hit5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1c7351eb-e9fd-4020-9436-d31b7d797e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    {\n",
    "        \"anchor\": q,\n",
    "        \"positive\": d\n",
    "    }\n",
    "    for q, d in train_pairs\n",
    "]\n",
    "train_ds = Dataset.from_list(train_data)\n",
    "\n",
    "val_data = [\n",
    "    {\n",
    "        \"anchor\": q,\n",
    "        \"positive\": d\n",
    "    }\n",
    "    for q, d in val_pairs\n",
    "]\n",
    "val_ds = Dataset.from_list(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6df40986-1c11-4b4f-8462-1610ecc66bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n",
      "PyTorch: setting up devices\n",
      "average_tokens_across_devices is True but world size is 1. Setting it to False automatically.\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n",
      "                                                                     "
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 4\n",
    "max_steps_cap = 120\n",
    "warmup_ratio = 0.05\n",
    "\n",
    "steps_per_epoch = min(math.ceil(len(train_ds) / batch_size), max_steps_cap)\n",
    "total_steps = steps_per_epoch * epochs\n",
    "\n",
    "loss_fn = losses.MultipleNegativesRankingLoss(st_model)\n",
    "\n",
    "training_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"st-encoder-qlora-out\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    num_train_epochs=epochs,\n",
    "    max_steps=total_steps,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    report_to=\"none\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_drop_last=True,\n",
    "    dataloader_num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=st_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    loss=loss_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87867341-baf0-4a26-969b-0b0b795d6278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=10.47GB, reserved=14.91GB, peak=13.94GB\n",
      "NVML used=16.27GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "9cfe28ba-a91a-47f3-83e2-75d91afb297e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipped Embedding(250002, 1024, padding_idx=1): 244.142578125M params\n",
      "skipped Embedding(514, 1024, padding_idx=1): 244.64453125M params\n",
      "skipped Embedding(1, 1024): 244.6455078125M params\n",
      "skipped: 244.6455078125M params\n",
      "***** Running training *****\n",
      "  Num examples = 2,000\n",
      "  Num Epochs = 40\n",
      "  Instantaneous batch size per device = 32\n",
      "  Training with DataParallel so batch size has been adjusted to: 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 256\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 315\n",
      "  Number of trainable parameters = 7,110,656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [315/315 7:13:06, Epoch 39/40]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.198900</td>\n",
       "      <td>0.055596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.152000</td>\n",
       "      <td>0.057347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.057545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.059949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.137200</td>\n",
       "      <td>0.059919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.117700</td>\n",
       "      <td>0.059835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 500\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=315, training_loss=0.24426108958229187, metrics={'train_runtime': 26072.3939, 'train_samples_per_second': 3.093, 'train_steps_per_second': 0.012, 'total_flos': 0.0, 'train_loss': 0.24426108958229187, 'epoch': 39.38709677419355})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a40f2658-6dd3-4d03-9e35-9e73ffda1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[QLoRA after train] allocated=10.48GB, reserved=19.19GB, peak=18.06GB\n",
      "NVML used=20.56GB / total=23.99GB\n"
     ]
    }
   ],
   "source": [
    "gpu_mem(\"QLoRA after train\"); nvidia_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ecf6f259-b021-4f02-813c-a02c625045bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/viv232/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/0dc5580a448e4284468b8909bae50fa925907bc5/config.json\n",
      "Model config XLMRobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"XLMRobertaModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"xlm-roberta\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.56.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250002\n",
      "}\n",
      "\n",
      "tokenizer config file saved in st-encoder-qlora-out/final_model/tokenizer_config.json\n",
      "Special tokens file saved in st-encoder-qlora-out/final_model/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "st_model.save(\"st-encoder-qlora-out/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2b251a2-11b8-4e5a-b330-4538c2314819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit@5: 0.972\n"
     ]
    }
   ],
   "source": [
    "q_vecs_after = embed(queries, st_model)\n",
    "d_vecs_after = embed(docs, st_model)\n",
    "\n",
    "sims_after = np.matmul(q_vecs_after, d_vecs_after.T)\n",
    "\n",
    "k = min(5, sims_after.shape[1])\n",
    "topk_idx = np.argpartition(-sims_after, kth=k-1, axis=1)[:, :k]\n",
    "\n",
    "true_idx = np.arange(len(val_pairs))\n",
    "hits = (topk_idx == true_idx[:, None]).any(axis=1)\n",
    "hit5 = hits.mean()\n",
    "\n",
    "print(f\"Hit@5: {hit5:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79da5bc1-435d-41f8-b547-0781aa557909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
